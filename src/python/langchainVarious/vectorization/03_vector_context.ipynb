{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [  \n",
    "    # Botanic context  \n",
    "    \"I planted a maple tree in the backyard.\",  \n",
    "    \"The tree provides shade during hot summer days.\",  \n",
    "    \"We sat under the old oak tree.\",  \n",
    "    \"The tree blossoms in the spring.\",  \n",
    "    \"The apple tree bears fruit every autumn.\",  \n",
    "    \"The tree's roots were deep in the ground.\",  \n",
    "    \"The tree's leaves turned yellow and fell off.\",  \n",
    "    \"The pine tree stood tall in the forest.\",  \n",
    "    \"The willow tree hung over the pond.\",  \n",
    "    \"The tree was cut down to make room for new construction.\",  \n",
    "      \n",
    "    # Computer programming context  \n",
    "    \"The binary tree is a fundamental data structure in computer science.\",  \n",
    "    \"Each node in the tree stores a piece of data.\",  \n",
    "    \"The tree structure allows efficient search and sort operations.\",  \n",
    "    \"The tree is traversed in a pre-order, in-order, or post-order manner.\",  \n",
    "    \"A balanced binary tree offers optimal performance.\",  \n",
    "    \"The tree's root node has no parent.\",  \n",
    "    \"Each node in the tree has a link to its parent and children.\",  \n",
    "    \"The tree's leaf nodes have no children.\",  \n",
    "    \"A tree in computer science is not necessarily rooted.\",  \n",
    "    \"The tree algorithm was implemented recursively.\",  \n",
    "      \n",
    "    # Family tree context  \n",
    "    \"My family tree traces back to the 16th century.\",  \n",
    "    \"I am researching my family tree.\",  \n",
    "    \"My family tree has branches all over the world.\",  \n",
    "    \"The family tree shows our genealogy.\",  \n",
    "    \"I found an interesting ancestor in our family tree.\",  \n",
    "    \"My family tree is quite complex.\",  \n",
    "    \"Our family tree includes several notable individuals.\",  \n",
    "    \"The family tree reveals our heritage.\",  \n",
    "    \"I discovered distant relatives through the family tree.\",  \n",
    "    \"The family tree helps us understand our roots.\",  \n",
    "]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "transformers_cache = os.environ.get('TRANSFORMERS_CACHE')\n",
    "print(transformers_cache)\n",
    "\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', cache_folder=transformers_cache)\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', cache_folder=transformers_cache)\n",
    "#model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1', cache_folder=transformers_cache)\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', cache_folder=transformers_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Now we want to transform all sentences with bert\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# calculate the module of all vectors in embeddings\n",
    "vectors_module = np.linalg.norm(embeddings, axis=1)\n",
    "# Verify if the vector are unit vector or not. This is important\n",
    "# because the cosine similarity is defined as the dot product\n",
    "# between two unit vectors. If the vectors are not unit vectors\n",
    "# the cosine similarity cannot be calculated with dot product\n",
    "pprint(vectors_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#and now I want to print a matrix with the cosine similarity of each sentence with each other sentence\n",
    "#df = pd.DataFrame(cosine_similarity(embeddings))\n",
    "df = pd.DataFrame(util.cos_sim(embeddings, embeddings))\n",
    "dfdot = pd.DataFrame(util.dot_score(embeddings, embeddings))\n",
    "\n",
    "df.columns = range(len(sentences))\n",
    "df.index = range(len(sentences))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import umap\n",
    "import altair as alt\n",
    "\n",
    "# declare a function to plot the embeddings\n",
    "def plot_embeddings(sentences, embeddings):\n",
    "\n",
    "    panda_sentences = pd.DataFrame({'text':sentences})\n",
    "\n",
    "    # UMAP reduces dimension to a plottable 2DE\n",
    "    reducer = umap.UMAP(n_neighbors=2)\n",
    "    umap_embeds = reducer.fit_transform(embeddings)\n",
    "    # create a dataframe with the umap embeddings and the corresponding sentences\n",
    "    df_plot = pd.DataFrame({'x': umap_embeds[:, 0], 'y': umap_embeds[:, 1], 'text': panda_sentences['text']})\n",
    "\n",
    "    # create the interactive scatter plot with labels\n",
    "    return alt.Chart(df_plot, width=1100, height=600).mark_circle(size=60).encode(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        tooltip=['text']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot the embeddings\n",
    "chart  = plot_embeddings(sentences, embeddings)\n",
    "\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#print out the value of the environment variable OPENAI_API_KEY\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")   \n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "embeddings_ada = []\n",
    "for sentence in sentences:\n",
    "    response = openai.Embedding.create(\n",
    "        input=sentence,\n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    embeddings_ada.append(response['data'][0]['embedding'])\n",
    "    \n",
    "# Now we can plot the embeddings\n",
    "chart  = plot_embeddings(sentences, embeddings_ada)\n",
    "chart.interactive()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cosine_similarity(embeddings_ada))\n",
    "df.columns = range(len(sentences))\n",
    "df.index = range(len(sentences))\n",
    "#print only the first sentence\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
