{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [  \n",
    "    # Botanic context  \n",
    "    \"I planted a maple tree in the backyard.\",  \n",
    "    \"The tree provides shade during hot summer days.\",  \n",
    "    \"We sat under the old oak tree.\",  \n",
    "    \"The tree blossoms in the spring.\",  \n",
    "    \"The apple tree bears fruit every autumn.\",  \n",
    "    \"The tree's roots were deep in the ground.\",  \n",
    "    \"The tree's leaves turned yellow and fell off.\",  \n",
    "    \"The pine tree stood tall in the forest.\",  \n",
    "    \"The willow tree hung over the pond.\",  \n",
    "    \"The tree was cut down to make room for new construction.\",  \n",
    "      \n",
    "    # Computer programming context  \n",
    "    \"The binary tree is a fundamental data structure in computer science.\",  \n",
    "    \"Each node in the tree stores a piece of data.\",  \n",
    "    \"The tree structure allows efficient search and sort operations.\",  \n",
    "    \"The tree is traversed in a pre-order, in-order, or post-order manner.\",  \n",
    "    \"A balanced binary tree offers optimal performance.\",  \n",
    "    \"The tree's root node has no parent.\",  \n",
    "    \"Each node in the tree has a link to its parent and children.\",  \n",
    "    \"The tree's leaf nodes have no children.\",  \n",
    "    \"A tree in computer science is not necessarily rooted.\",  \n",
    "    \"The tree algorithm was implemented recursively.\",  \n",
    "      \n",
    "    # Family tree context  \n",
    "    \"My family tree traces back to the 16th century.\",  \n",
    "    \"I am researching my family tree.\",  \n",
    "    \"My family tree has branches all over the world.\",  \n",
    "    \"The family tree shows our genealogy.\",  \n",
    "    \"I found an interesting ancestor in our family tree.\",  \n",
    "    \"My family tree is quite complex.\",  \n",
    "    \"Our family tree includes several notable individuals.\",  \n",
    "    \"The family tree reveals our heritage.\",  \n",
    "    \"I discovered distant relatives through the family tree.\",  \n",
    "    \"The family tree helps us understand our roots.\",  \n",
    "]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from visualization_utils import openai_plot_2D, plot_embeddings\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "transformers_cache = os.environ.get('TRANSFORMERS_CACHE')\n",
    "print(transformers_cache)\n",
    "\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', cache_folder=transformers_cache)\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', cache_folder=transformers_cache)\n",
    "#model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1', cache_folder=transformers_cache)\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', cache_folder=transformers_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Now we want to transform all sentences with bert\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# calculate the module of all vectors in embeddings\n",
    "vectors_module = np.linalg.norm(embeddings, axis=1)\n",
    "vectors_module = np.round(vectors_module, decimals=5)\n",
    "\n",
    "# Verify if the vectors are normalized vectors or not. This is important\n",
    "# because the cosine similarity is defined as the dot product\n",
    "# between two normalized vectors. \n",
    "pprint(vectors_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#and now I want to print a matrix with the cosine similarity of each sentence with each other sentence\n",
    "#df = pd.DataFrame(cosine_similarity(embeddings))\n",
    "df = pd.DataFrame(util.cos_sim(embeddings, embeddings))\n",
    "dfdot = pd.DataFrame(util.dot_score(embeddings, embeddings))\n",
    "\n",
    "df.columns = range(len(sentences))\n",
    "df.index = range(len(sentences))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot the embeddings\n",
    "chart  = plot_embeddings(sentences, embeddings)\n",
    "chart.interactive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai_plot_2D(sentences, embeddings, show_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#print out the value of the environment variable OPENAI_API_KEY\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT\")\n",
    ")\n",
    "\n",
    "embeddings_ada = []\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=sentences,\n",
    "        model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "for data in response.data:\n",
    "    embeddings_ada.append(data.embedding)\n",
    "\n",
    "# Now we can plot the embeddings\n",
    "chart = plot_embeddings(sentences, embeddings_ada)\n",
    "chart.interactive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_plot_2D(sentences, embeddings_ada, show_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use openai new models\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv('OPENAI_API_KEY_NOT_AZURE')\n",
    "\n",
    "response_te3_large = client.embeddings.create(\n",
    "    input=sentences,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "embeddings_te3_large = []\n",
    "for data in response_te3_large.data:\n",
    "    embeddings_te3_large.append(data.embedding)\n",
    "\n",
    "openai_plot_2D(sentences, embeddings_te3_large, show_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now reduce dimensions\n",
    "# now use openai new models\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv('OPENAI_API_KEY_NOT_AZURE')\n",
    "\n",
    "response_te3_large_512 = client.embeddings.create(\n",
    "    input=sentences,\n",
    "    dimensions=512,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "embeddings_te3_large_512 = []\n",
    "for data in response_te3_large_512.data:\n",
    "    embeddings_te3_large_512.append(data.embedding)\n",
    "\n",
    "openai_plot_2D(sentences, embeddings_te3_large_512, show_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now reduce dimensions a lot\n",
    "# now use openai new models\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv('OPENAI_API_KEY_NOT_AZURE')\n",
    "\n",
    "response_te3_large_128 = client.embeddings.create(\n",
    "    input=sentences,\n",
    "    dimensions=128,\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "embeddings_te3_large_128 = []\n",
    "for data in response_te3_large_128.data:\n",
    "    embeddings_te3_large_128.append(data.embedding)\n",
    "\n",
    "openai_plot_2D(sentences, embeddings_te3_large_128, show_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart  = plot_embeddings(sentences, embeddings_te3_large_128)\n",
    "\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#last time with bert\n",
    "\n",
    "# Encode the shuffled sentences using the model\n",
    "embeddings_bert = model.encode(sentences)\n",
    "\n",
    "# Plot the embeddings in 2D\n",
    "openai_plot_2D(embeddings_bert)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
