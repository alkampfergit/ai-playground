{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "transformers_cache = os.environ.get('TRANSFORMERS_CACHE')\n",
    "print(transformers_cache)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if you installed torch correctly and your GPU is available\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the name of the pre-trained transformer model to use\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "\n",
    "# Define additional arguments to pass to the HuggingFaceEmbeddings constructor\n",
    "model_kwargs = {'device': 'cuda'} # or 'cpu'\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Create an instance of the HuggingFaceEmbeddings class using the specified model name and arguments\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is a Python script that demonstrates how to use the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module to create embeddings for text data using a pre-trained transformer model. \n",
    "\n",
    "First, the script imports the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module. This class is used to create embeddings for text data using pre-trained transformer models from the Hugging Face model hub.\n",
    "\n",
    "Next, the script defines a `model_name` variable that specifies the name of the pre-trained transformer model to use. In this case, the model is `sentence-transformers/distiluse-base-multilingual-cased-v1`, which is a multilingual sentence embedding model based on the DistilBERT architecture.\n",
    "\n",
    "The script also defines two dictionaries: `model_kwargs` and `encode_kwargs`. These dictionaries are used to pass additional arguments to the `HuggingFaceEmbeddings` constructor. In this case, `model_kwargs` specifies that the model should be loaded onto the GPU if available, and `encode_kwargs` specifies that the embeddings should not be normalized.\n",
    "\n",
    "Finally, the script creates an instance of the `HuggingFaceEmbeddings` class using the `model_name`, `model_kwargs`, and `encode_kwargs` variables. This instance can then be used to encode text data into embeddings using the `encode` method.\n",
    "\n",
    "Overall, this code demonstrates how to use the `HuggingFaceEmbeddings` class to create embeddings for text data using a pre-trained transformer model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ElasticVectorSearch\n",
    "\n",
    "elastic_vector_search = ElasticVectorSearch(\n",
    "    elasticsearch_url=\"http://localhost:9201\",\n",
    "    index_name=\"test_index\",\n",
    "    embedding=hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Define the path to the directory containing the PDF files\n",
    "pdf_dir = 'S:\\\\OneDrive\\\\Documentation\\\\HumbleBundle\\\\Security apress'\n",
    "\n",
    "# Create a list to store the loaded PDF documents\n",
    "pdf_docs = []\n",
    "\n",
    "# Traverse the directory tree and load the PDF files\n",
    "for root, dirs, files in os.walk(pdf_dir):\n",
    "    for file in tqdm(files, desc=\"Loading PDF files\", unit=\"file\"):\n",
    "        if file.endswith('.pdf'):  \n",
    "            pdf_path = os.path.join(root, file)\n",
    "            pdf_loader = PyPDFLoader(pdf_path)\n",
    "            pdf_doc = pdf_loader.load()\n",
    "            pdf_docs.append(pdf_doc)\n",
    "\n",
    "# Print the number of loaded PDF documents\n",
    "print(f\"Loaded {len(pdf_docs)} PDF documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(f\"Loaded {len(pdf_docs)} PDF documents\")\n",
    "doc = pdf_docs[0] \n",
    "print(f\"first doc has {len(doc)} elements\")\n",
    "firstElement = doc[0]\n",
    "print(f\"firstElement is of type {type(firstElement)}\")\n",
    "print(f\"firstElement has {firstElement.metadata} medatata\")\n",
    "print(f\"firstElement has {firstElement.page_content} page content\")\n",
    "\n",
    "total_pages = 0\n",
    "for doc in pdf_docs:\n",
    "    total_pages += len(doc)\n",
    "    # Count the number of pages that have more than 1000 characters\n",
    "    num_long_pages = 0\n",
    "    for doc in pdf_docs:\n",
    "        for page in doc:\n",
    "            if len(page.page_content) > 1000:\n",
    "                num_long_pages += 1\n",
    "\n",
    "print(f\"Number of pages with more than 1000 characters: {num_long_pages}\")\n",
    "print(f\"Total number of pages: {total_pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator = \" \", chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "number_of_docs = 0\n",
    "for doc in pdf_docs:\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "    number_of_docs += len(docs)\n",
    "    print(f\"for document {doc[0].metadata.source} we have {len(docs)} embeddings from a total of {len(doc)} pages\")\n",
    "    db = elastic_vector_search.add_documents(docs)\n",
    "    # for chunk in text_splitter.split_documents(doc):\n",
    "    #     splitting.append(chunk)\n",
    "\n",
    "pprint(f\"total number of embedded documents is {number_of_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator = \" \",chunk_size=400, chunk_overlap=0)\n",
    "first_doc = pdf_docs[0] \n",
    "\n",
    "print(f\"first doc has {len(first_doc)} pages\")\n",
    "page = first_doc[50]\n",
    "print(f\"page number 50 has {len(page.page_content)} characters\")\n",
    "#pprint(page.page_content)\n",
    "\n",
    "page_chunks = text_splitter.split_text(page.page_content)\n",
    "print(f\"page has {len(page_chunks)} chunks\")\n",
    "pprint(page_chunks)\n",
    "\n",
    "# chunks = text_splitter.split_documents(first_doc)\n",
    "# print(f\"document has {len(chunks)} chunks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
