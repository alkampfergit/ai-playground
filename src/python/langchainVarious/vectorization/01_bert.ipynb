{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:\\HuggingFace\\cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0388562   0.01854845 -0.04066142 ...  0.01009196 -0.01660533\n",
      "  -0.00138948]\n",
      " [-0.00059496 -0.00924198 -0.05870508 ...  0.01638781  0.0150957\n",
      "  -0.04368325]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "transformers_cache = os.environ.get('TRANSFORMERS_CACHE')\n",
    "print(transformers_cache)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.3600, 0.4760, 0.1246],\n",
      "        [0.3600, 1.0000, 0.5832, 0.1338],\n",
      "        [0.4760, 0.5832, 1.0000, 0.1140],\n",
      "        [0.1246, 0.1338, 0.1140, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [\n",
    "    \"Yesterday I've played with my cat, I had a pleasant evening\", \n",
    "    \"I really like going to hike with my dog, it's a lot of fun\",\n",
    "    \"Swimming with my dog in the pool, really fun evening\",\n",
    "    \"I love elasticsearch capabilities to search for similar sentences\"]\n",
    "embeddings = model.encode(sentences)\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "similarity_matrix = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Verify if you installed torch correctly and your GPU is available\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the name of the pre-trained transformer model to use\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "\n",
    "# Define additional arguments to pass to the HuggingFaceEmbeddings constructor\n",
    "model_kwargs = {'device': 'cuda'} # or 'cpu'\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Create an instance of the HuggingFaceEmbeddings class using the specified model name and arguments\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is a Python script that demonstrates how to use the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module to create embeddings for text data using a pre-trained transformer model. \n",
    "\n",
    "First, the script imports the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module. This class is used to create embeddings for text data using pre-trained transformer models from the Hugging Face model hub.\n",
    "\n",
    "Next, the script defines a `model_name` variable that specifies the name of the pre-trained transformer model to use. In this case, the model is `sentence-transformers/distiluse-base-multilingual-cased-v1`, which is a multilingual sentence embedding model based on the DistilBERT architecture.\n",
    "\n",
    "The script also defines two dictionaries: `model_kwargs` and `encode_kwargs`. These dictionaries are used to pass additional arguments to the `HuggingFaceEmbeddings` constructor. In this case, `model_kwargs` specifies that the model should be loaded onto the GPU if available, and `encode_kwargs` specifies that the embeddings should not be normalized.\n",
    "\n",
    "Finally, the script creates an instance of the `HuggingFaceEmbeddings` class using the `model_name`, `model_kwargs`, and `encode_kwargs` variables. This instance can then be used to encode text data into embeddings using the `encode` method.\n",
    "\n",
    "Overall, this code demonstrates how to use the `HuggingFaceEmbeddings` class to create embeddings for text data using a pre-trained transformer model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import ElasticVectorSearch\n",
    "\n",
    "elastic_vector_search = ElasticVectorSearch(\n",
    "    elasticsearch_url=\"http://localhost:9201\",\n",
    "    index_name=\"test_index\",\n",
    "    embedding=hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF files: 100%|██████████| 18/18 [00:55<00:00,  3.09s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 PDF documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Define the path to the directory containing the PDF files\n",
    "pdf_dir = 'S:\\\\OneDrive\\\\Documentation\\\\HumbleBundle\\\\Security apress'\n",
    "\n",
    "# Create a list to store the loaded PDF documents\n",
    "pdf_docs = []\n",
    "\n",
    "# Traverse the directory tree and load the PDF files\n",
    "for root, dirs, files in os.walk(pdf_dir):\n",
    "    for file in tqdm(files, desc=\"Loading PDF files\", unit=\"file\"):\n",
    "        if file.endswith('.pdf'):  \n",
    "            pdf_path = os.path.join(root, file)\n",
    "            pdf_loader = PyPDFLoader(pdf_path)\n",
    "            pdf_doc = pdf_loader.load()\n",
    "            pdf_docs.append(pdf_doc)\n",
    "\n",
    "# Print the number of loaded PDF documents\n",
    "print(f\"Loaded {len(pdf_docs)} PDF documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Loaded 18 PDF documents'\n",
      "first doc has 239 elements\n",
      "firstElement is of type <class 'langchain.schema.Document'>\n",
      "firstElement has {'source': 'S:\\\\OneDrive\\\\Documentation\\\\HumbleBundle\\\\Security apress\\\\appliedcryptographyinnetandazurekeyvault.pdf', 'page': 0} medatata\n",
      "firstElement has Applied \n",
      "Cryptography in \n",
      ".NET and Azure  \n",
      "Key Vault\n",
      "A Practical Guide to Encryption in  \n",
      ".NET and .NET Core\n",
      "—\n",
      "Stephen Haunts\n",
      "Foreword by Troy Hunt page content\n",
      "Number of pages with more than 1000 characters: 4215\n",
      "Total number of pages: 6120\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(f\"Loaded {len(pdf_docs)} PDF documents\")\n",
    "doc = pdf_docs[0] \n",
    "print(f\"first doc has {len(doc)} elements\")\n",
    "firstElement = doc[0]\n",
    "print(f\"firstElement is of type {type(firstElement)}\")\n",
    "print(f\"firstElement has {firstElement.metadata} medatata\")\n",
    "print(f\"firstElement has {firstElement.page_content} page content\")\n",
    "\n",
    "total_pages = 0\n",
    "for doc in pdf_docs:\n",
    "    total_pages += len(doc)\n",
    "    # Count the number of pages that have more than 1000 characters\n",
    "    num_long_pages = 0\n",
    "    for doc in pdf_docs:\n",
    "        for page in doc:\n",
    "            if len(page.page_content) > 1000:\n",
    "                num_long_pages += 1\n",
    "\n",
    "print(f\"Number of pages with more than 1000 characters: {num_long_pages}\")\n",
    "print(f\"Total number of pages: {total_pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:301: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  version_num = client.info()[\"version\"][\"number\"][0]\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:306: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  client.indices.create(index=index_name, body={\"mappings\": mapping})\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:208: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  bulk(self.client, requests)\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:211: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.client.indices.refresh(index=self.index_name)\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:189: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.client.indices.get(index=self.index_name)\n",
      "Created a chunk of size 1206, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'total number of embedded documents is 12257'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator = \" \", chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "number_of_docs = 0\n",
    "for doc in pdf_docs:\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "    number_of_docs += len(docs)\n",
    "    # print(f\"for document {doc[0].metadata} we have {len(docs)} embeddings from a total of {len(doc)} pages\")\n",
    "    db = elastic_vector_search.add_documents(docs)\n",
    "    # for chunk in text_splitter.split_documents(doc):\n",
    "    #     splitting.append(chunk)\n",
    "\n",
    "pprint(f\"total number of embedded documents is {number_of_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator = \" \",chunk_size=400, chunk_overlap=0)\n",
    "first_doc = pdf_docs[0] \n",
    "\n",
    "print(f\"first doc has {len(first_doc)} pages\")\n",
    "page = first_doc[50]\n",
    "print(f\"page number 50 has {len(page.page_content)} characters\")\n",
    "#pprint(page.page_content)\n",
    "\n",
    "page_chunks = text_splitter.split_text(page.page_content)\n",
    "print(f\"page has {len(page_chunks)} chunks\")\n",
    "pprint(page_chunks)\n",
    "\n",
    "# chunks = text_splitter.split_documents(first_doc)\n",
    "# print(f\"document has {len(chunks)} chunks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
