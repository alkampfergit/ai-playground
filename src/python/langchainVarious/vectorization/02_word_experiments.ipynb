{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from pprint import pprint\n",
    "gmodel = api.load('word2vec-google-news-300') # load pre-trained word2vec model\n",
    "\n",
    "model_info = api.info('word2vec-google-news-300')\n",
    "model_path = model_info['file_name']\n",
    "pprint(model_info)\n",
    "print(f'Model path is = {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = gmodel.most_similar(positive=['woman', 'king'], negative=['man']) # find the most similar word to woman + king - man\n",
    "print(result[0]) # print the first result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gmodel.most_similar(positive=['Tokyo', 'france'], negative=['paris']) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gmodel.most_similar(positive=['rome', 'paris'], negative=['italy']) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the idea is that taking a `wolf` (feral dog) removing the `dog` part and add a `cat` part we *will end with some sort of feral cat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gmodel.most_similar(positive=['cat', 'wolf'], negative=['dog']) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gmodel.most_similar(positive=['weapon', 'wood'], negative=[]) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if (device == 'cpu'):\n",
    "    # ok we are using CPU but we could use apple metal instead\n",
    "    if torch.backends.mps.is_available():\n",
    "        print (\"Using MPS\")\n",
    "        # device = torch.device('mps')  # use M1 chip if available\n",
    "    else:\n",
    "        print (\"Using CPU\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.nn.functional.cosine_similarity(a, b).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Load word list (assuming you've saved it as a newline-separated text file)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/dwyl/english-words/master/words.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('words.txt', 'w') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "with open(\"words.txt\", \"r\") as f:\n",
    "    words = f.readlines()\n",
    "    \n",
    "words = [word.strip() for word in words]\n",
    "\n",
    "word_embeddings = []\n",
    "\n",
    "# Create batches\n",
    "BATCH_SIZE = 512\n",
    "num_batches = int(np.ceil(len(words) / BATCH_SIZE))\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    batch = words[i*BATCH_SIZE: (i+1)*BATCH_SIZE]\n",
    "    \n",
    "    # Tokenizing in batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    inputs.to(device)\n",
    "    # Passing through the model\n",
    "    outputs = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # Extract embeddings for each word in the batch\n",
    "    for j in range(len(batch)):\n",
    "        word_embedding = outputs[j].mean(dim=0).detach()\n",
    "        word_embeddings.append(word_embedding)\n",
    "    \n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Move embeddings back to CPU if needed\n",
    "word_embeddings = [emb.cpu() for emb in word_embeddings]\n",
    "\n",
    "pprint (word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute analogy vector\n",
    "# get embeddings\n",
    "inputs = tokenizer([\"woman\", \"king\", \"man\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs.to(device) # move to GPU if available\n",
    "outputs = model(**inputs).last_hidden_state\n",
    "\n",
    "woman = outputs[0].mean(dim=0) # average the embeddings for tokens of \"woman\"\n",
    "king = outputs[1].mean(dim=0)  # average the embeddings for tokens of \"king\"\n",
    "man = outputs[2].mean(dim=0)   # average the embeddings for tokens of \"man\"\n",
    "\n",
    "del inputs\n",
    "torch.cuda.empty_cache()\n",
    "# compute analogy vector\n",
    "analogy_vector = woman + king - man\n",
    "pprint(f\"analogy_vcetor device = {analogy_vector.device}\")\n",
    "analogy_vector.to(\"cpu\")\n",
    "\n",
    "# Find most similar word\n",
    "similarities = {}\n",
    "for word, embedding in zip(words, word_embeddings):\n",
    "    similarity = cosine_similarity(analogy_vector.unsqueeze(0), embedding.unsqueeze(0))\n",
    "    similarities[word] = similarity\n",
    "\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_similarities[0])  # This should print the most similar word and its similarity score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
