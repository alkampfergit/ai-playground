{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_dataset': 'Google News (about 100 billion words)',\n",
      " 'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
      " 'description': 'Pre-trained vectors trained on a part of the Google News '\n",
      "                'dataset (about 100 billion words). The model contains '\n",
      "                '300-dimensional vectors for 3 million words and phrases. The '\n",
      "                'phrases were obtained using a simple data-driven approach '\n",
      "                \"described in 'Distributed Representations of Words and \"\n",
      "                \"Phrases and their Compositionality' \"\n",
      "                '(https://code.google.com/archive/p/word2vec/).',\n",
      " 'file_name': 'word2vec-google-news-300.gz',\n",
      " 'file_size': 1743563840,\n",
      " 'license': 'not found',\n",
      " 'num_records': 3000000,\n",
      " 'parameters': {'dimension': 300},\n",
      " 'parts': 1,\n",
      " 'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
      "               'https://arxiv.org/abs/1301.3781',\n",
      "               'https://arxiv.org/abs/1310.4546',\n",
      "               'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
      " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'}\n",
      "Model path is = word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from pprint import pprint\n",
    "gmodel = api.load('word2vec-google-news-300') # load pre-trained word2vec model\n",
    "\n",
    "model_info = api.info('word2vec-google-news-300')\n",
    "model_path = model_info['file_name']\n",
    "pprint(model_info)\n",
    "print(f'Model path is = {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519),\n",
      " ('monarch', 0.6189674139022827),\n",
      " ('princess', 0.5902431011199951),\n",
      " ('crown_prince', 0.5499460697174072),\n",
      " ('prince', 0.5377321839332581)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = gmodel.most_similar(positive=['woman', 'king'], negative=['man']) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Japan', 0.6101208925247192),\n",
      " ('Osaka', 0.5625056624412537),\n",
      " ('Japanese', 0.5529288649559021),\n",
      " ('Nagoya', 0.552433431148529),\n",
      " ('Seoul', 0.5309796929359436)]\n"
     ]
    }
   ],
   "source": [
    "result = gmodel.most_similar(positive=['Tokyo', 'france'], negative=['paris']) # find the most similar word to france - paris + Tokyo\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italy', 0.519952118396759),\n",
      " ('european', 0.5075845718383789),\n",
      " ('italian', 0.5057743191719055),\n",
      " ('epl', 0.490744411945343),\n",
      " ('spain', 0.4888668656349182)]\n"
     ]
    }
   ],
   "source": [
    "result = gmodel.most_similar(positive=['rome', 'france'], negative=['paris']) \n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the idea is that taking a `wolf` (feral dog) removing the `dog` part and add a `cat` part we *will end with some sort of feral cat*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wolves', 0.6855551600456238),\n",
      " ('lynx', 0.6411743760108948),\n",
      " ('gray_wolf', 0.6279042363166809),\n",
      " ('gray_wolves', 0.5749408006668091),\n",
      " ('wolverine', 0.5593723058700562)]\n"
     ]
    }
   ],
   "source": [
    "result = gmodel.most_similar(positive=['cat', 'wolf'], negative=['dog']) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('splitting_maul', 0.631394624710083),\n",
      " ('double_barreled_shotgun', 0.5669865608215332),\n",
      " ('weapons', 0.5578944683074951),\n",
      " ('muzzle_loading_rifle', 0.5393639802932739),\n",
      " ('jacketed_bullets', 0.538790225982666)]\n"
     ]
    }
   ],
   "source": [
    "result = gmodel.most_similar(positive=['weapon', 'wood'], negative=[]) # find the most similar word to woman + king - man\n",
    "pprint(result[:5]) # print the first result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if (device == 'cpu'):\n",
    "    # ok we are using CPU but we could use apple metal instead\n",
    "    if torch.backends.mps.is_available():\n",
    "        print (\"Using MPS\")\n",
    "        # device = torch.device('mps')  # use M1 chip if available\n",
    "    else:\n",
    "        print (\"Using CPU\")\n",
    "\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.nn.functional.cosine_similarity(a, b).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Number of words: 466550'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Load word list (assuming you've saved it as a newline-separated text file)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/dwyl/english-words/master/words.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('words.txt', 'w') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "with open(\"words.txt\", \"r\") as f:\n",
    "    words = f.readlines()\n",
    "    \n",
    "words = [word.strip() for word in words]\n",
    "pprint(f'Number of words: {len(words)}')\n",
    "\n",
    "word_embeddings = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912/912 [01:01<00:00, 14.85it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create batches\n",
    "BATCH_SIZE = 512\n",
    "num_batches = int(np.ceil(len(words) / BATCH_SIZE))\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    batch = words[i*BATCH_SIZE: (i+1)*BATCH_SIZE]\n",
    "    \n",
    "    # Tokenizing in batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "    inputs.to(device)\n",
    "    # Passing through the model\n",
    "    outputs = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # Extract embeddings for each word in the batch\n",
    "    for j in range(len(batch)):\n",
    "        word_embedding = outputs[j].mean(dim=0).detach()\n",
    "        word_embeddings.append(word_embedding)\n",
    "    \n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Move embeddings back to CPU if needed\n",
    "word_embeddings = [emb.cpu() for emb in word_embeddings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0517, -0.0343,  0.0778,  0.0019,  0.0024, -0.0077,  0.0327,  0.0210,\n",
      "        -0.0436,  0.0353, -0.0041,  0.1117, -0.0267,  0.0047,  0.0114, -0.0065,\n",
      "         0.0367,  0.0344,  0.0537, -0.0209,  0.0631,  0.0037,  0.0315, -0.0138,\n",
      "         0.0600,  0.0095,  0.0313,  0.1338,  0.0926, -0.0573, -0.0919,  0.0053,\n",
      "        -0.0204,  0.0067,  0.0179,  0.1597, -0.0904,  0.0343,  0.0243,  0.0141,\n",
      "        -0.0225, -0.0650,  0.0827, -0.0232, -0.0026, -0.0793, -0.0171, -0.0397,\n",
      "         0.0730,  0.0091,  0.0205, -0.0574, -0.0126,  0.0094, -0.0509, -0.0643,\n",
      "        -0.0481,  0.0694,  0.0808,  0.0363,  0.0066,  0.0393,  0.0196,  0.0173,\n",
      "        -0.1359,  0.0173,  0.0638,  0.0540,  0.0068, -0.0518, -0.0224,  0.0717,\n",
      "         0.0062, -0.0082,  0.0596,  0.0338, -0.0106,  0.0206,  0.0546, -0.0179,\n",
      "         0.0127, -0.0339,  0.0689, -0.0053,  0.0164, -0.1020,  0.0288, -0.0183,\n",
      "         0.0498,  0.1885,  0.0189, -0.0025, -0.0225, -0.0240, -0.0098,  0.0206,\n",
      "         0.0132, -0.0240,  0.1074, -0.0231, -0.0214, -0.0133, -0.0236,  0.0189,\n",
      "         0.0029,  0.1212, -0.0579,  0.0212, -0.0004,  0.0064, -0.0035,  0.0212,\n",
      "         0.0769, -0.0398, -0.0952, -0.0133, -0.0061, -0.0804, -0.0557, -0.0492,\n",
      "         0.0250,  0.0934, -0.0249,  0.0590, -0.0115,  0.0499, -0.0905,  0.0130,\n",
      "        -0.0308,  0.0631,  0.0258,  0.0070,  0.0365, -0.0463,  0.0453,  0.0269,\n",
      "         0.1045,  0.0093,  0.0895,  0.0440, -0.0011,  0.0235, -0.0888, -0.0274,\n",
      "         0.0218, -0.0032, -0.0972, -0.0131, -0.0617,  0.0191, -0.0280, -0.0565,\n",
      "         0.0523,  0.0200, -0.0154,  0.0605, -0.0143, -0.0030,  0.0313, -0.0180,\n",
      "         0.0376,  0.0880, -0.0831, -0.0976, -0.1057,  0.0058,  0.0207, -0.0229,\n",
      "        -0.0143,  0.0863,  0.0155, -0.0473,  0.0537,  0.1115,  0.0549, -0.0088,\n",
      "        -0.0494, -0.0733, -0.0513, -0.0140, -0.0603,  0.0555,  0.0849,  0.0052,\n",
      "        -0.0667,  0.0410, -0.0598,  0.0175, -0.2039, -0.0569,  0.0390,  0.0105,\n",
      "         0.0298, -0.0280,  0.0526, -0.0312,  0.0571,  0.0370,  0.1004, -0.0209,\n",
      "         0.0019,  0.0334,  0.0349,  0.0609, -0.0639,  0.0495, -0.0154, -0.0183,\n",
      "        -0.0559, -0.0636,  0.0231, -0.0778, -0.0555,  0.0792, -0.0034, -0.0634,\n",
      "        -0.0378, -0.0224,  0.0344,  0.0815,  0.0856, -0.0730,  0.0636, -0.0315,\n",
      "         0.0149, -0.0080, -0.0461, -0.0046,  0.0722,  0.1320, -0.1055,  0.0124,\n",
      "         0.0168,  0.0239, -0.1364, -0.0469,  0.0400, -0.0031,  0.0711, -0.0361,\n",
      "        -0.0132,  0.0480,  0.0352, -0.0368, -0.0272, -0.0090, -0.0361,  0.0038,\n",
      "         0.1755, -0.0782,  0.0860,  0.0663,  0.1327, -0.0412,  0.0215, -0.0222,\n",
      "         0.0152,  0.0079, -0.0032, -0.0059,  0.0319, -0.0150,  0.0389,  0.0098,\n",
      "         0.0043,  0.0956,  0.0333,  0.0350,  0.0278, -0.0067,  0.0059,  0.0268,\n",
      "        -0.0490,  0.0149, -0.0034, -0.0407,  0.0173,  0.0073,  0.1083, -0.0260,\n",
      "        -0.0405, -0.0146,  0.0112,  0.0178, -0.1900,  0.0068,  0.0291,  0.0420,\n",
      "         0.0081,  0.0599, -0.0469, -0.0018, -0.1412, -0.0360,  0.0794, -0.0026,\n",
      "        -0.0065, -0.0130, -0.0669, -0.0073,  0.0582, -0.0030, -0.0106,  0.0142,\n",
      "        -0.0610, -0.0298,  0.0441, -0.0112,  0.0051, -0.0759,  0.0843,  0.0015,\n",
      "         0.0209,  0.0771,  0.0180,  0.0466,  0.0779,  0.0330, -0.0087,  0.0050,\n",
      "        -0.0841, -0.0812,  0.0009,  0.0443, -0.0087,  0.0183,  0.0564,  0.0574,\n",
      "        -0.0544, -0.0170,  0.0097,  0.0216, -0.0574,  0.0005,  0.0757,  0.0602,\n",
      "         0.0406,  0.0762, -0.0182, -0.0381, -0.0282, -0.0551,  0.0302,  0.0116,\n",
      "        -0.0181, -0.0278, -0.0435,  0.0138,  0.0238,  0.1043,  0.0374,  0.0213,\n",
      "         0.0860,  0.0678, -0.0408, -0.0374, -0.0012,  0.0511,  0.0325,  0.0278,\n",
      "        -0.0082,  0.0268, -0.0883,  0.0121,  0.0287, -0.0617, -0.0206, -0.0077,\n",
      "        -0.1022, -0.0734,  0.0028,  0.0112, -0.0241, -0.0458, -0.0497,  0.1234,\n",
      "         0.0934,  0.0595, -0.0067,  0.0209, -0.0533,  0.0010, -0.0146, -0.0406,\n",
      "         0.0502,  0.0116,  0.0385,  0.0551, -0.0617,  0.0275,  0.0032, -0.0140,\n",
      "         0.1107,  0.0240,  0.0367,  0.0062, -0.0171, -0.0219, -0.0052,  0.0122,\n",
      "        -0.0136,  0.0491, -0.0218,  0.0046, -0.0193, -0.0163,  0.0602, -0.0871,\n",
      "         0.0011, -0.0858, -0.0357,  0.0258,  0.0526,  0.0548,  0.1175,  0.0732,\n",
      "         0.0094,  0.0541, -0.0003,  0.0210, -0.0477,  0.0084,  0.1117,  0.0594,\n",
      "         0.0730, -0.0168,  0.0294, -0.1310, -0.0248, -0.0115,  0.0135,  0.0654,\n",
      "        -0.0625,  0.0214,  0.0287,  0.0587, -0.0289, -0.0653, -0.0557, -0.0504,\n",
      "        -0.0786,  0.0123, -0.0112, -0.0396, -0.0426, -0.0353, -0.0150, -0.0306,\n",
      "        -0.0311,  0.0864, -0.0270,  0.0522, -0.0613,  0.0543, -0.0542,  0.0880,\n",
      "        -0.0020, -0.0365,  0.0026,  0.0460,  0.0062, -0.0118, -0.0007,  0.0087,\n",
      "         0.0243,  0.0090, -0.0674,  0.0308,  0.0472, -0.0209, -0.0752,  0.0596,\n",
      "        -0.0694, -0.0290,  0.0787, -0.0463,  0.0958, -0.0147, -0.0295, -0.0609,\n",
      "        -0.0402,  0.0151,  0.0443, -0.0027,  0.0130, -0.0175, -0.1275,  0.0301,\n",
      "        -0.0762,  0.0405,  0.0345, -0.0489, -0.0859, -0.0418,  0.0484,  0.0631,\n",
      "        -0.0700, -0.0257, -0.0064, -0.0364,  0.0472,  0.0016, -0.0017,  0.0452,\n",
      "        -0.0473, -0.0059,  0.0368, -0.0438, -0.0364,  0.0527, -0.0141,  0.0235,\n",
      "        -0.0317,  0.0124, -0.0060, -0.0365,  0.0168, -0.0619,  0.0449,  0.0636,\n",
      "        -0.0032, -0.0628,  0.0026,  0.0159, -0.0537,  0.0050, -0.0148,  0.0299,\n",
      "        -0.0011, -0.0570,  0.0094, -0.0698,  0.1022,  0.0270, -0.0098,  0.0788,\n",
      "         0.0652, -0.0919,  0.0557, -0.1019,  0.0066,  0.0242,  0.0113,  0.0029,\n",
      "        -0.0129,  0.0499, -0.1030, -0.0517, -0.0151,  0.0551, -0.0881, -0.1215,\n",
      "         0.0108, -0.0861, -0.0794,  0.1068, -0.0260,  0.0195,  0.0018, -0.0051,\n",
      "         0.0541,  0.0340,  0.0632,  0.0712, -0.0325,  0.0053, -0.0171, -0.0546,\n",
      "         0.0327, -0.0298,  0.0020,  0.0620, -0.0288,  0.0038, -0.0908, -0.0036,\n",
      "         0.0207, -0.0155, -0.0186,  0.0066,  0.0184, -0.0469, -0.0132,  0.1006,\n",
      "         0.1006, -0.0309, -0.0621,  0.1136,  0.0456, -0.0366,  0.0194,  0.0045,\n",
      "        -0.0807, -0.0423, -0.0221,  0.0637,  0.0162, -0.0087, -0.1508,  0.0279,\n",
      "        -0.0705, -0.0222, -0.0368, -0.0871, -0.0084, -0.0062, -0.0077, -0.0088,\n",
      "        -0.0487, -0.0846, -0.0265,  0.0570, -0.1410,  0.0260, -0.0651, -0.0374,\n",
      "        -0.0498,  0.0511, -0.0243, -0.0240, -0.0109, -0.0550, -0.0324, -0.0027,\n",
      "         0.0579,  0.0429,  0.0052,  0.0201, -0.0256, -0.0207,  0.0106, -0.0225,\n",
      "         0.0014,  0.0295,  0.0350, -0.0165, -0.0392,  0.0959, -0.0503,  0.0260,\n",
      "        -0.0256, -0.1033, -0.0506,  0.0318,  0.0628,  0.0196,  0.0094, -0.0110,\n",
      "         0.0531, -0.0127, -0.0098, -0.0708,  0.0043,  0.0141,  0.0004,  0.0606,\n",
      "        -0.0606,  0.1446, -0.0404, -0.0077,  0.0163, -0.0395, -0.0537, -0.0233,\n",
      "         0.0599,  0.0916, -0.0303,  0.0702, -0.0609, -0.0021, -0.0166,  0.0756,\n",
      "         0.0519,  0.0015,  0.0239,  0.1048,  0.0178, -0.0201,  0.1179,  0.0107,\n",
      "         0.0132, -0.0690, -0.0211,  0.0617, -0.0330,  0.0377, -0.0367, -0.0178,\n",
      "         0.0613,  0.0160, -0.0560,  0.0332,  0.1164,  0.0018, -0.0519,  0.0213,\n",
      "        -0.0450,  0.0147, -0.0639, -0.0191,  0.0462, -0.0252,  0.0467,  0.0419,\n",
      "         0.0496, -0.0457, -0.0786, -0.0009,  0.0195,  0.0988, -0.0507, -0.0601,\n",
      "        -0.0987, -0.0826,  0.1007, -0.0898, -0.0136, -0.0233,  0.0394,  0.1020,\n",
      "        -0.0816,  0.0963, -0.0840, -0.0213,  0.0272, -0.0166,  0.0472, -0.0516,\n",
      "        -0.0038, -0.0465, -0.0087, -0.0628,  0.0889, -0.0131,  0.0476,  0.0114,\n",
      "         0.1743,  0.0212, -0.0128,  0.0336,  0.0067,  0.0199,  0.0583, -0.0224,\n",
      "        -0.1258,  0.0353,  0.0438,  0.0467, -0.0802,  0.0234,  0.0683, -0.0211,\n",
      "         0.0135, -0.0666,  0.0204, -0.0190,  0.1013,  0.0386, -0.0293,  0.1704,\n",
      "         0.0016,  0.0460,  0.0592, -0.0115, -0.0943, -0.0401,  0.0828, -0.0089])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pprint (word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def find_similar_words(source, subtract, add):\n",
    "\n",
    "    # Compute analogy vector\n",
    "    # get embeddings\n",
    "    inputs = tokenizer([source, subtract, add], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs.to(device) # move to GPU if available\n",
    "    outputs = model(**inputs).last_hidden_state\n",
    "\n",
    "    source_tensor = outputs[0].mean(dim=0).detach() # average the embeddings for tokens of \"woman\"\n",
    "    subtract_tensor = outputs[1].mean(dim=0).detach()  # average the embeddings for tokens of \"king\"\n",
    "    add_tensor = outputs[2].mean(dim=0) .detach()  # average the embeddings for tokens of \"man\"\n",
    "\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # compute analogy vector\n",
    "    analogy_vector = source_tensor - subtract_tensor + add_tensor\n",
    "    # pprint(f\"analogy_vector device = {analogy_vector.device}\")\n",
    "    analogy_vector = analogy_vector.to(\"cpu\")\n",
    "    # pprint(f\"analogy_vector device = {analogy_vector.device}\")\n",
    "\n",
    "    # Find most similar word\n",
    "    similarities = {}\n",
    "\n",
    "    pprint(f\"Brute forcing on a total of {len(words)} words\")\n",
    "\n",
    "    # Assuming word_embeddings is a 2D tensor where each row is the embedding of a word\n",
    "    word_embeddings_tensor = torch.stack(word_embeddings)\n",
    "\n",
    "    # Unsqueeze analogy_vector to match the dimensions of word_embeddings_tensor\n",
    "    analogy_vector_unsqueezed = analogy_vector.unsqueeze(0)\n",
    "\n",
    "    # Calculate cosine similarity for all words at once\n",
    "    similarities = torch.nn.functional.cosine_similarity(analogy_vector_unsqueezed, word_embeddings_tensor).tolist()\n",
    "\n",
    "    word_similarities = list(zip(words, similarities))\n",
    "\n",
    "    sorted_similarities = sorted(word_similarities, key=lambda x: x[1], reverse=True)\n",
    "    pprint(sorted_similarities[:10])  # This should print the most similar word and its similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Brute forcing on a total of 466550 words'\n",
      "[('Queen', 0.6610273122787476),\n",
      " ('reginas', 0.6440814733505249),\n",
      " ('Queena', 0.6367193460464478),\n",
      " ('king-ridden', 0.6314623951911926),\n",
      " ('queenhood', 0.6295172572135925),\n",
      " ('king-whiting', 0.6291139125823975),\n",
      " ('reginae', 0.6278669238090515),\n",
      " ('rei', 0.6261332035064697),\n",
      " ('kral', 0.6235274076461792),\n",
      " (\"queen's\", 0.6195412874221802)]\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"king\", \"man\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Brute forcing on a total of 466550 words'\n",
      "[('Japan', 0.677340030670166),\n",
      " ('Tokio', 0.6668616533279419),\n",
      " ('Japanee', 0.6527673006057739),\n",
      " ('Japans', 0.6449138522148132),\n",
      " ('Tokyo', 0.6312811970710754),\n",
      " ('France', 0.6312498450279236),\n",
      " ('Japanesy', 0.6257008910179138),\n",
      " ('Nippon', 0.6213622093200684),\n",
      " ('Tokyoite', 0.6124879717826843),\n",
      " ('Japanesque', 0.6107927560806274)]\n"
     ]
    }
   ],
   "source": [
    "find_similar_words(\"France\", \"Paris\", \"Tokyo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
