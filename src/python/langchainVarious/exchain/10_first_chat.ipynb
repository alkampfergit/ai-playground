{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First chat\n",
    "\n",
    "First part of the chain, just setup api key (on `azure` in this example). This cell prints the name of the base deployment just to verify that we correctly loaded the .env file.\n",
    "\n",
    "Remember you need a .env file with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxxxxx\n",
    "OPENAI_API_BASE=https://alkopenai2.openai.azure.com/\n",
    "HUGGINGFACEHUB_API_TOKEN=xxxxxxx\n",
    "PINECONE_KEY=xxxxxxx\n",
    "PINECONE_ENV=us-west1-gcp-free\n",
    "SERPAPI_API_KEY=xxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alkopenai2.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Remember that you need to set the OPENAI_API_BASE to point openai to your specific deployment.\n",
    "\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print base to verify you are pointint to the right deployment\n",
    "print(openai.api_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always I'm creating various llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI  \n",
    "\n",
    "# Pay attention that gpt35 and gpt4 are chat based llms\n",
    "gpt35 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "gpt4 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt4\", \n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "# this is not a Chat model this is a text model\n",
    "davinci = AzureOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"text-davinci-003\", \n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the example starts to behave differently, we starts using a real `chat object` from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Memory for the conversation is chat_memory=ChatMessageHistory(messages=[]) '\n",
      " \"output_key=None input_key=None return_messages=False human_prefix='Human' \"\n",
      " \"ai_prefix='AI' memory_key='history'\")\n",
      "\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\"\n",
      "'--------------------------'\n",
      "' Your name is Gian Maria.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from pprint import pprint\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=davinci, # use davinci that is not a real chat model but it simply works.\n",
    "    verbose=False #Feel free to set to true to look for internal working, in this example is not necessary\n",
    ")\n",
    "\n",
    "# Actually the conversation has some actual memory\n",
    "pprint(f\"Memory for the conversation is {conversation.memory}\")\n",
    "\n",
    "result = conversation.predict(input = \"Hi my name is Gian Maria, what is your name?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we verified that the conversation has some memory, this means that the calls to the LLM is done using some form of memory. Now lets try **to dump some memory content to verify what is inside it**. As you can verify it contains simply the list of the messages, like previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi my name is Gian Maria, what is your name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\", additional_kwargs={}, example=False),\n",
      " HumanMessage(content='What is my name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=' Your name is Gian Maria.', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "pprint (conversation.memory.chat_memory.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to use a different form of `memory called ConversationBufferMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\"\n",
      "'--------------------------'\n",
      "' Your name is Gian Maria.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=davinci, # use davinci that is not a real chat model but it simply works.\n",
    "    memory=memory,\n",
    "    verbose=False #Feel free to set to true to look for internal working, in this example is not necessary\n",
    ")\n",
    "\n",
    "result = conversation.predict(input = \"Hi my name is Gian Maria, what is your name?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is pretty much the same, it seems that the ConverstaionBufferMemory works similarly to the ChatMessageHistory form of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi my name is Gian Maria, what is your name?', additional_kwargs={}, example=False), AIMessage(content=\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\", additional_kwargs={}, example=False), HumanMessage(content='What is my name?', additional_kwargs={}, example=False), AIMessage(content=' Your name is Gian Maria.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history')\n",
      "'------------------'\n",
      "[HumanMessage(content='Hi my name is Gian Maria, what is your name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\", additional_kwargs={}, example=False),\n",
      " HumanMessage(content='What is my name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=' Your name is Gian Maria.', additional_kwargs={}, example=False)]\n",
      "'------------------'\n",
      "('Human: Hi my name is Gian Maria, what is your name?\\n'\n",
      " \"AI:  Hi Gian Maria, my name is AI-1. It's nice to meet you.\\n\"\n",
      " 'Human: What is my name?\\n'\n",
      " 'AI:  Your name is Gian Maria.')\n"
     ]
    }
   ],
   "source": [
    "pprint (memory) #Please verify output, you can see that this wraps a ChatMessageHistory.\n",
    "pprint(\"------------------\")\n",
    "pprint (memory.chat_memory.messages)\n",
    "pprint(\"------------------\")\n",
    "pprint (memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we want to use a memory explicitly **instead of avoiding using the default one?**. One of the answer is the ability to tune the chat memory manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hi my name is Gian Maria, what is your name?\\n'\n",
      "            \"AI:  Hi Gian Maria, my name is AI-1. It's nice to meet you.\\n\"\n",
      "            'Human: What is my name?\\n'\n",
      "            'AI:  Your name is Gian Maria.'}\n"
     ]
    }
   ],
   "source": [
    "# you can get the history\n",
    "history =memory.load_memory_variables({})\n",
    "pprint (history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Your name is John.'\n",
      "'Human: What is my name?\\nAI:  Your name is John.'\n"
     ]
    }
   ],
   "source": [
    "# i can reset the memory\n",
    "memory.clear()\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)\n",
    "pprint(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Human: Hy my name is Gian Maria\\nAI: Hi Gian Maria my name is Alfred!'\n",
      "[HumanMessage(content='Hy my name is Gian Maria', additional_kwargs={}, example=False),\n",
      " AIMessage(content='Hi Gian Maria my name is Alfred!', additional_kwargs={}, example=False)]\n",
      "------------------\n",
      "\" Your name is Gian Maria and my name is Alfred. It's nice to meet you!\"\n"
     ]
    }
   ],
   "source": [
    "memory.clear()\n",
    "\n",
    "# I can preload the memory with a conversation, so we can start from a known state\n",
    "memory.save_context({\"input\" : \"Hy my name is Gian Maria\"}, {\"output\" : \"Hi Gian Maria my name is Alfred!\"})\n",
    "pprint(memory.buffer)\n",
    "pprint (memory.chat_memory.messages)\n",
    "\n",
    "print (\"------------------\")\n",
    "result = conversation.predict(input= \"What is my name? What is your name?\")\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I apologize for my previous response. Let me correct myself. The total '\n",
      " 'number of common letters in our names is 4. The unique common letters are '\n",
      " \"'a', 'r', 'i', and 'm'. Gian Maria and Alfred share these four letters.\")\n"
     ]
    }
   ],
   "source": [
    "result = conversation.predict(input= \"Sum the number of common letters of our names, give a detailed explanation of the answer.\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool stuff for this kind of interaction is that we can change anything **as an example I can change llm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I apologize for the confusion in my previous responses. To clarify, I will '\n",
      " 'now count the frequency of each common letter in both names and provide the '\n",
      " 'sum.\\n'\n",
      " '\\n'\n",
      " 'Gian Maria: G (1), I (1), A (2), N (1), M (1), R (1)\\n'\n",
      " 'Alfred: A (1), L (1), F (1), R (1), E (1), D (1)\\n'\n",
      " '\\n'\n",
      " 'The common letters are A, R, and I, with the following frequencies in each '\n",
      " 'name:\\n'\n",
      " '- A: Gian Maria (2), Alfred (1)\\n'\n",
      " '- R: Gian Maria (1), Alfred (1)\\n'\n",
      " '- I: Gian Maria (1), Alfred (0)\\n'\n",
      " '\\n'\n",
      " 'Summing these frequencies, we get:\\n'\n",
      " '- A: 2 + 1 = 3\\n'\n",
      " '- R: 1 + 1 = 2\\n'\n",
      " '- I: 1 + 0 = 1\\n'\n",
      " '\\n'\n",
      " 'So, the total number of common letters in our names, considering their '\n",
      " 'frequencies, is 3 + 2 + 1 = 6.')\n"
     ]
    }
   ],
   "source": [
    "conversation.llm = gpt4\n",
    "result = conversation.predict(input= \"Sum the number of common letters of our names, give a detailed explanation of the answer.\")\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
