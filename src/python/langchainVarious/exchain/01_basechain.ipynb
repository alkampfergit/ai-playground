{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates and direct call api\n",
    "\n",
    "First part of the chain, just setup api key (on `azure` in this example). This cell prints the name of the base deployment just to verify that we correctly loaded the .env file.\n",
    "\n",
    "Remember you need a .env file with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxxxxx\n",
    "OPENAI_API_BASE=https://alkopenai2.openai.azure.com/\n",
    "HUGGINGFACEHUB_API_TOKEN=xxxxxxx\n",
    "PINECONE_KEY=xxxxxxx\n",
    "PINECONE_ENV=us-west1-gcp-free\n",
    "SERPAPI_API_KEY=xxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alkopenai2.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Remember that you need to set the OPENAI_API_BASE to point openai to your specific deployment.\n",
    "\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print base to verify you are pointint to the right deployment\n",
    "print(openai.api_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to work langchain nees to work with LLM, you start creating some LLM in this situation we use OpenAI in azure in different flavors.\n",
    "\n",
    "The important aspect is that some of the llm models are `chat models` and others are `text models`. The chat models are trained to answer questions and the text models are trained to generate text. You will use both of these in **different situation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI  \n",
    "\n",
    "# Pay attention that gpt35 and gpt4 are chat based llms\n",
    "gpt35 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "gpt4 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt4\", \n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "# this is not a Chat model this is a text model\n",
    "davinci = AzureOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"text-davinci-003\", \n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain first function: ability to create a `prompt` with placeholders to better manage the prompt. The key here is simply the ability to create a string with placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence', 'target_lang']\n",
      "('Type of the chat_sample_prompt is a list of 1 first element type is  <class '\n",
      " \"'langchain.schema.HumanMessage'>\")\n",
      "('Type of the prompt_template is <class '\n",
      " \"'langchain.prompts.prompt.PromptTemplate'> and input variables are \"\n",
      " \"['sentence', 'target_lang']\")\n",
      "\"Type of the sample_prompt is <class 'str'>\"\n",
      "('Translate sentence surrounded by triple ticks to de.\\n'\n",
      " '\\n'\n",
      " '```This is a simple experiment to test langchain```\\n')\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# pay attention we are not using python f\" style of strings.\n",
    "prompt_template_string = \"\"\"Translate sentence surrounded by triple ticks to {target_lang}.\n",
    "\n",
    "```{sentence}```\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt object from the template \n",
    "# super important THE MODEL IS OF TYPE CHATPROMPTTEMPLATE\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "# now you have a complex template object that as an example has input variables\n",
    "pprint(chat_prompt_template.messages[0].input_variables)\n",
    "\n",
    "# now you can generate a real prompt substituing input variables.\n",
    "chat_sample_prompt = chat_prompt_template.format_messages(target_lang=\"de\", sentence=\"This is a simple experiment to test langchain\")\n",
    "pprint(f\"Type of the chat_sample_prompt is a list of {len(chat_sample_prompt)} first element type is  {type(chat_sample_prompt[0])}\")\n",
    "\n",
    "# you can create a simple template\n",
    "prompt_template = PromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "pprint(f\"Type of the prompt_template is {type(prompt_template)} and input variables are {prompt_template.input_variables}\")\n",
    "sample_prompt = prompt_template.format(target_lang=\"de\", sentence=\"This is a simple experiment to test langchain\")\n",
    "\n",
    "pprint(f\"Type of the sample_prompt is {type(sample_prompt)}\")\n",
    "pprint(sample_prompt   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Type of first element: <class 'langchain.schema.HumanMessage'>\"\n",
      "Translate sentence surrounded by triple ticks to de.\n",
      "\n",
      "```This is a simple experiment to test langchain```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this will show that the chat sample prompt is a list of objects and each object has a type\n",
    "# and a content attribute that contains the text of the prompt\n",
    "first_element = chat_sample_prompt[0]\n",
    "pprint(f\"Type of first element: {type(first_element)}\")\n",
    "print(first_element.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chat Models` can accept the textPrompt object, that is basically an array, and simply return another piece of the conversation. We are actually creating a `prompt` with a single message and the chat model will answer with another piece of object.\n",
    "\n",
    "You will see that the textPrompt is a `list` containing HumanMessages or AIMessages. This is the standard way to dialogate with a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textPrompt is of type <class 'list'> and first element is of type <class 'langchain.schema.HumanMessage'>\n",
      "Result is of type <class 'langchain.schema.AIMessage'>\n",
      "AIMessage(content='```Dies ist ein einfaches Experiment, um Langkette zu testen```', additional_kwargs={}, example=False)\n",
      "('Content of the answer is = ```Dies ist ein einfaches Experiment, um '\n",
      " 'Langkette zu testen```')\n"
     ]
    }
   ],
   "source": [
    "# now you can call an api through the wrapper\n",
    "print(f\"textPrompt is of type {type(chat_sample_prompt)} and first element is of type {type(chat_sample_prompt[0])}\")  \n",
    "result = gpt35(chat_sample_prompt)  \n",
    "print(f\"Result is of type {type(result)}\")\n",
    "pprint(result)  \n",
    "\n",
    "#actually it is more useful to print the content\n",
    "pprint(f\"Content of the answer is = {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davinci is a simple completion model, so it will simply complete my prompt. \n",
    "1. I need to pass the prompt as string not the entire prompt (only chain models supports    the entire prompt)\n",
    "2. I will simply return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dies ist ein einfaches Experiment, um Langchain zu testen.\n",
      "'\\nDies ist ein einfaches Experiment, um Langchain zu testen.'\n"
     ]
    }
   ],
   "source": [
    "# if you have a ChatPromptTemplate you need to pass a single string content.\n",
    "result = davinci(chat_sample_prompt[0].content)\n",
    "print(result)\n",
    "\n",
    "# it is more standard to have a standard PromptTemplate that uses .format to create\n",
    "# a sample string\n",
    "result = davinci(sample_prompt)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically `ChatPromptTemplate` and `PromptTemplate` are used respectively with models of type Chat or Text. Please always remember this distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting using a chat model\n",
    "\n",
    "Now we will start using a chat model to simulate what is happening in standard chat GPT. Situation becomes more interesting because we can now interact with `AIMessage, HumanMessage and SystemMessage` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Gian Maria, how can I assist you today?' additional_kwargs={} example=False\n",
      "content=\"I'm sorry, but as an AI language model, I do not have access to your personal information such as your name.\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "# This example is just wrong, each call is a different conversation, a different api\n",
    "# call. \n",
    "result1 = gpt35([HumanMessage(content =      \"Hy my name is gian maria\")])\n",
    "print(result1)\n",
    "result2 = gpt35([HumanMessage(content =      \"What is my name\")])\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call to API have no context, each call is a new context, a new call and the AI does not know nothing about previous calls. It is up to the caller to provide `a list of the previous calls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"AI answer with an object of type <class 'langchain.schema.AIMessage'>\"\n",
      "'Hello Gian Maria! How may I assist you today?'\n",
      "'What is my name?'\n",
      "('Your name is Gian Maria, as you mentioned earlier. Is there anything else I '\n",
      " 'can help you with?')\n"
     ]
    }
   ],
   "source": [
    "conversation = []\n",
    "conversation.append(SystemMessage(content=\"You are an helpful UI assistant\"))\n",
    "conversation.append(HumanMessage(content=\"Hello My name is Gian Maria\"))\n",
    "ai_answer = gpt35(conversation)\n",
    "pprint(f\"AI answer with an object of type {type(ai_answer)}\")\n",
    "pprint(ai_answer.content)\n",
    "#Now we append the AI answer to the conversation\n",
    "conversation.append(ai_answer)\n",
    "\n",
    "# now we can continue the conversation asking for my name\n",
    "message = HumanMessage(content=\"What is my name?\")\n",
    "pprint(message.content)\n",
    "conversation.append(message)\n",
    "ai_answer = gpt35(conversation)\n",
    "pprint(ai_answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create a method that will simply print the whole chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat(conversation):\n",
    "    for i in range(len(conversation)):\n",
    "        # each element is of three different type, human, context and ai we need\n",
    "        # to distinguish them based on type of object\n",
    "        if isinstance(conversation[i], SystemMessage):\n",
    "            print(\"Context: \" + conversation[i].content)\n",
    "        elif isinstance(conversation[i], HumanMessage):\n",
    "            print(\"Human: \" + conversation[i].content)\n",
    "        else:\n",
    "            print(\"AI: \" + conversation[i].content)\n",
    "\n",
    "def start_chat(model, context, first_human_message):\n",
    "    conversation = [SystemMessage(content=context)]\n",
    "    conversation.append(HumanMessage(content=first_human_message))\n",
    "    conversation.append(model(conversation))\n",
    "    return conversation\n",
    "\n",
    "def continue_chat(model, conversation, human_message):\n",
    "    conversation.append(HumanMessage(content=human_message))\n",
    "    conversation.append(model(conversation))\n",
    "    return conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous methods allows you to interact easier with the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria\n",
      "AI: Hello Gian Maria! How can I assist you today?\n",
      "'------------------'\n",
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria\n",
      "AI: Hello Gian Maria! How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is Gian Maria.\n"
     ]
    }
   ],
   "source": [
    "context = start_chat(gpt35, \"you are an helpful ai\", \"Hi my name is Gian Maria\")\n",
    "print_chat(context)\n",
    "\n",
    "pprint(\"------------------\")\n",
    "# now you can prosecute the chat\n",
    "context = continue_chat(gpt35, context, \"What is my name?\")\n",
    "print_chat(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are in full control of everythign you can manipulate everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria. What is your name?\n",
      "AI: Hello Gian Maria! I am an AI language model developed by OpenAI, and I don't have a name. You can call me OpenAI if you'd like! How can I assist you today?\n",
      "'------------------'\n",
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria. What is your name?\n",
      "AI: Hi Gian Maria, my name is Ambrogio.\n",
      "Human: Which are the letters that are common between our names? Please explain citing yours and my name.\n",
      "AI: The letters that are common between our names, Gian Maria and Ambrogio are \"a\", \"i\" and \"m\". Gian Maria has the letters \"a\", \"i\" and \"m\" in it, and Ambrogio has the letters \"a\", \"i\" and \"m\" in it as well.\n"
     ]
    }
   ],
   "source": [
    "context = start_chat(gpt35, \"you are an helpful ai\", \"Hi my name is Gian Maria. What is your name?\")\n",
    "print_chat(context)\n",
    "\n",
    "pprint(\"------------------\")\n",
    "# now you can prosecute the chat but you can change anything like the previous answer of the ai\n",
    "context[2].content = \"Hi Gian Maria, my name is Ambrogio.\"\n",
    "context = continue_chat(gpt35, context, \"Which are the letters that are common between our names? Please explain citing yours and my name.\")\n",
    "print_chat(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use a ChatPromptTemplate for a complex conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As I mentioned earlier, my name is Al2000. And yes, I would be glad to help you with any computer problems you may be experiencing. What seems to be the issue?' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {ai_name}.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"Hi my name is {user_name}.\"),\n",
    "    AIMessagePromptTemplate.from_template(\"Hi {user_name}, nice to meet you, my name is {ai_name} and I'm here to help you!\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    ai_name=\"Al2000\",\n",
    "    user_name=\"Gian Maria\",\n",
    "    user_input=\"I'm having some problems with my computer, can you help me? But first, what's your name?\"\n",
    ")\n",
    "\n",
    "answer = gpt35(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change some other paramter of the query directly in the call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI bot. Your name is Al2000.', additional_kwargs={}), HumanMessage(content='Hi my name is Gian Maria.', additional_kwargs={}, example=False), AIMessage(content=\"Hi Gian Maria, nice to meet you, my name is Al2000 and I'm here to help you!\", additional_kwargs={}, example=False), HumanMessage(content='I have a computer that suddenly does not boot to window. \\n    Monitor turns on, windows logo appears but then nothing happens. \\n    What can I do to diagnose the problem?', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format_messages(\n",
    "    ai_name=\"Al2000\",\n",
    "    user_name=\"Gian Maria\",\n",
    "    user_input=\"\"\"I have a computer that suddenly does not boot to window. \n",
    "    Monitor turns on, windows logo appears but then nothing happens. \n",
    "    What can I do to diagnose the problem?\"\"\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "answer = gpt35(prompt, temperature=2)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
