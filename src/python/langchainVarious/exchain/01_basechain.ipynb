{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First part of the chain, just setup api key (on `azure` in this example). This cell prints the name of the base deployment just to verify that we correctly loaded the .env file.\n",
    "\n",
    "Remember you need a .env file with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxxxxx\n",
    "OPENAI_API_BASE=https://alkopenai2.openai.azure.com/\n",
    "HUGGINGFACEHUB_API_TOKEN=xxxxxxx\n",
    "PINECONE_KEY=xxxxxxx\n",
    "PINECONE_ENV=us-west1-gcp-free\n",
    "SERPAPI_API_KEY=xxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Remember that you need to set the OPENAI_API_BASE to point openai to your specific deployment.\n",
    "\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print base to verify you are pointint to the right deployment\n",
    "print(openai.api_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to work langchain nees to work with LLM, you start creating some LLM in this situation we use OpenAI in azure in different flavors.\n",
    "\n",
    "The important aspect is that some of the llm models are `chat models` and others are `text models`. The chat models are trained to answer questions and the text models are trained to generate text. You will use both of these in **different situation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI  \n",
    "\n",
    "# Pay attention that gpt35 and gpt4 are chat based llms\n",
    "gpt35 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "gpt4 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt4\", \n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "# this is not a Chat model this is a text model\n",
    "davinci = AzureOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"text-davinci-003\", \n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain first function: ability to create a `prompt` with placeholders to better manage the prompt. The key here is simply the ability to create a string with placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# pay attention we are not using python f\" style of strings.\n",
    "prompt_template_string = \"\"\"Translate sentence surrounded by triple ticks to {target_lang}.\n",
    "\n",
    "```{sentence}```\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt object from the template \n",
    "# super important THE MODEL IS OF TYPE CHATPROMPTTEMPLATE\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "# now you have a complex template object that as an example has input variables\n",
    "pprint(chat_prompt_template.messages[0].input_variables)\n",
    "\n",
    "# now you can generate a real prompt substituing input variables.\n",
    "chat_sample_prompt = chat_prompt_template.format_messages(target_lang=\"de\", sentence=\"This is a simple experiment to test langchain\")\n",
    "pprint(f\"Type of the chat_sample_prompt is a list of {len(chat_sample_prompt)} first element type is  {type(chat_sample_prompt[0])}\")\n",
    "\n",
    "# you can create a simple template\n",
    "prompt_template = PromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "pprint(f\"Type of the prompt_template is {type(prompt_template)} and input variables are {prompt_template.input_variables}\")\n",
    "sample_prompt = prompt_template.format(target_lang=\"de\", sentence=\"This is a simple experiment to test langchain\")\n",
    "\n",
    "pprint(f\"Type of the sample_prompt is {type(sample_prompt)}\")\n",
    "pprint(sample_prompt   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show that the chat sample prompt is a list of objects and each object has a type\n",
    "# and a content attribute that contains the text of the prompt\n",
    "first_element = chat_sample_prompt[0]\n",
    "pprint(f\"Type of first element: {type(first_element)}\")\n",
    "print(first_element.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chat Models` can accept the textPrompt object, that is basically an array, and simply return another piece of the conversation. We are actually creating a `prompt` with a single message and the chat model will answer with another piece of object.\n",
    "\n",
    "You will see that the textPrompt is a `list` containing HumanMessages or AIMessages. This is the standard way to dialogate with a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you can call an api through the wrapper\n",
    "print(f\"textPrompt is of type {type(chat_sample_prompt)} and first element is of type {type(chat_sample_prompt[0])}\")  \n",
    "result = gpt35(chat_sample_prompt)  \n",
    "print(f\"Result is of type {type(result)}\")\n",
    "pprint(result)  \n",
    "\n",
    "#actually it is more useful to print the content\n",
    "pprint(f\"Content of the answer is = {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davinci is a simple completion model, so it will simply complete my prompt. \n",
    "1. I need to pass the prompt as string not the entire prompt (only chain models supports    the entire prompt)\n",
    "2. I will simply return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a ChatPromptTemplate you need to pass a single string content.\n",
    "result = davinci(chat_sample_prompt[0].content)\n",
    "print(result)\n",
    "\n",
    "# it is more standard to have a standard PromptTemplate that uses .format to create\n",
    "# a sample string\n",
    "result = davinci(sample_prompt)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically `ChatPromptTemplate` and `PromptTemplate` are used respectively with models of type Chat or Text. Please always remember this distinction between the two."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
