{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First part of the chain, just setup api key (on `azure` in this example). This cell prints the name of the base deployment just to verify that we correctly loaded the .env file.\n",
    "\n",
    "Remember you need a .env file with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxxxxx\n",
    "OPENAI_API_BASE=https://alkopenai2.openai.azure.com/\n",
    "HUGGINGFACEHUB_API_TOKEN=xxxxxxx\n",
    "PINECONE_KEY=xxxxxxx\n",
    "PINECONE_ENV=us-west1-gcp-free\n",
    "SERPAPI_API_KEY=xxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Remember that you need to set the OPENAI_API_BASE to point openai to your specific deployment.\n",
    "\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print base to verify you are pointint to the right deployment\n",
    "print(openai.api_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to work langchain nees to work with LLM, you start creating some LLM in this situation we use OpenAI in azure in different flavors.\n",
    "\n",
    "The important aspect is that some of the llm models are `chat models` and others are `text models`. The chat models are trained to answer questions and the text models are trained to generate text. You will use both of these in **different situation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI  \n",
    "\n",
    "# Pay attention that gpt35 and gpt4 are chat based llms\n",
    "gpt35 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "gpt4 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt4\", \n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "# this is not a Chat model this is a text model\n",
    "davinci = AzureOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"text-davinci-003\", \n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain first function: ability to create a `prompt` with placeholders to better manage the prompt. The key here is simply the ability to create a string with placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# pay attention we are not using python f\" style of strings.\n",
    "prompt_template_string = \"\"\"Translate sentence surrounded by triple ticks to {target_lang}.\n",
    "\n",
    "```{sentence}```\n",
    "\"\"\"\n",
    "\n",
    "# Create a prompt object from the template \n",
    "# super important THE MODEL IS OF TYPE CHATPROMPTTEMPLATE\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "# now you have a complex template object that as an example has input variables\n",
    "pprint(chat_prompt_template.messages[0].input_variables)\n",
    "\n",
    "# now you can generate a real prompt substituing input variables.\n",
    "chat_sample_prompt = chat_prompt_template.format_messages(target_lang=\"de\", sentence=\"This is a simple experiment to test langchain\")\n",
    "pprint(f\"Type of the chat_sample_prompt is a list of {len(chat_sample_prompt)} first element type is  {type(chat_sample_prompt[0])}\")\n",
    "\n",
    "# you can create a simple template\n",
    "prompt_template = PromptTemplate.from_template(prompt_template_string)\n",
    "\n",
    "pprint(f\"Type of the prompt_template is {type(prompt_template)} and input variables are {prompt_template.input_variables}\")\n",
    "sample_prompt = prompt_template.format(target_lang=\"de\", sentence=\"This is a simple experiment to test langchain\")\n",
    "\n",
    "pprint(f\"Type of the sample_prompt is {type(sample_prompt)}\")\n",
    "pprint(sample_prompt   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will show that the chat sample prompt is a list of objects and each object has a type\n",
    "# and a content attribute that contains the text of the prompt\n",
    "first_element = chat_sample_prompt[0]\n",
    "pprint(f\"Type of first element: {type(first_element)}\")\n",
    "print(first_element.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chat Models` can accept the textPrompt object, that is basically an array, and simply return another piece of the conversation. We are actually creating a `prompt` with a single message and the chat model will answer with another piece of object.\n",
    "\n",
    "You will see that the textPrompt is a `list` containing HumanMessages or AIMessages. This is the standard way to dialogate with a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you can call an api through the wrapper\n",
    "print(f\"textPrompt is of type {type(chat_sample_prompt)} and first element is of type {type(chat_sample_prompt[0])}\")  \n",
    "result = gpt35(chat_sample_prompt)  \n",
    "print(f\"Result is of type {type(result)}\")\n",
    "pprint(result)  \n",
    "\n",
    "#actually it is more useful to print the content\n",
    "pprint(f\"Content of the answer is = {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davinci is a simple completion model, so it will simply complete my prompt. \n",
    "1. I need to pass the prompt as string not the entire prompt (only chain models supports    the entire prompt)\n",
    "2. I will simply return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a ChatPromptTemplate you need to pass a single string content.\n",
    "result = davinci(chat_sample_prompt[0].content)\n",
    "print(result)\n",
    "\n",
    "# it is more standard to have a standard PromptTemplate that uses .format to create\n",
    "# a sample string\n",
    "result = davinci(sample_prompt)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically `ChatPromptTemplate` and `PromptTemplate` are used respectively with models of type Chat or Text. Please always remember this distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting using a chat model\n",
    "\n",
    "Now we will start using a chat model to simulate what is happening in standard chat GPT. Situation becomes more interesting because we can now interact with `AIMessage, HumanMessage and SystemMessage` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Gian Maria! How may I assist you today?' additional_kwargs={} example=False\n",
      "content=\"I'm sorry, as an AI language model, I don't have access to your personal information. Please introduce yourself.\" additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "# This example is just wrong, each call is a different conversation, a different api\n",
    "# call. \n",
    "result1 = gpt35([HumanMessage(content =      \"Hy my name is gian maria\")])\n",
    "print(result1)\n",
    "result2 = gpt35([HumanMessage(content =      \"What is my name\")])\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call to API have no context, each call is a new context, a new call and the AI does not know nothing about previous calls. It is up to the caller to provide `a list of the previous calls`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"AI answer with an object of type <class 'langchain.schema.AIMessage'>\"\n",
      "'Hello Gian Maria! How can I assist you today?'\n",
      "'What is my name?'\n",
      "'Your name is Gian Maria.'\n"
     ]
    }
   ],
   "source": [
    "conversation = []\n",
    "conversation.append(SystemMessage(content=\"You are an helpful UI assistant\"))\n",
    "conversation.append(HumanMessage(content=\"Hello My name is Gian Maria\"))\n",
    "ai_answer = gpt35(conversation)\n",
    "pprint(f\"AI answer with an object of type {type(ai_answer)}\")\n",
    "pprint(ai_answer.content)\n",
    "#Now we append the AI answer to the conversation\n",
    "conversation.append(ai_answer)\n",
    "\n",
    "# now we can continue the conversation asking for my name\n",
    "message = HumanMessage(content=\"What is my name?\")\n",
    "pprint(message.content)\n",
    "conversation.append(message)\n",
    "ai_answer = gpt35(conversation)\n",
    "pprint(ai_answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create a method that will simply print the whole chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat(conversation):\n",
    "    for i in range(len(conversation)):\n",
    "        # each element is of three different type, human, context and ai we need\n",
    "        # to distinguish them based on type of object\n",
    "        if isinstance(conversation[i], SystemMessage):\n",
    "            print(\"Context: \" + conversation[i].content)\n",
    "        elif isinstance(conversation[i], HumanMessage):\n",
    "            print(\"Human: \" + conversation[i].content)\n",
    "        else:\n",
    "            print(\"AI: \" + conversation[i].content)\n",
    "\n",
    "def start_chat(model, context, first_human_message):\n",
    "    conversation = [SystemMessage(content=context)]\n",
    "    conversation.append(HumanMessage(content=first_human_message))\n",
    "    conversation.append(model(conversation))\n",
    "    return conversation\n",
    "\n",
    "def continue_chat(model, conversation, human_message):\n",
    "    conversation.append(HumanMessage(content=human_message))\n",
    "    conversation.append(model(conversation))\n",
    "    return conversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous methods allows you to interact easier with the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria\n",
      "AI: Hello Gian Maria! How can I assist you today?\n",
      "'------------------'\n",
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria\n",
      "AI: Hello Gian Maria! How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is Gian Maria, as you mentioned earlier. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "context = start_chat(gpt35, \"you are an helpful ai\", \"Hi my name is Gian Maria\")\n",
    "print_chat(context)\n",
    "\n",
    "pprint(\"------------------\")\n",
    "# now you can prosecute the chat\n",
    "context = continue_chat(gpt35, context, \"What is my name?\")\n",
    "print_chat(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are in full control of everythign you can manipulate everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria. What is your name?\n",
      "AI: Hello Gian Maria, I am an AI language model and I don't have a name. You can simply call me OpenAI. How can I assist you today?\n",
      "'------------------'\n",
      "Context: you are an helpful ai\n",
      "Human: Hi my name is Gian Maria. What is your name?\n",
      "AI: Hi Gian Maria, my name is Ambrogio.\n",
      "Human: Which are the letters that are common between our names? Please explain citing yours and my name.\n",
      "AI: The letters that are common between our names \"Gian Maria\" and \"Ambrogio\" are \"A\", \"I\", \"R\" and \"G\". \n",
      "\n",
      "Both our names contain the letter \"A\" at the beginning and end. The letter \"I\" appears twice in \"Gian Maria\" and once in \"Ambrogio\". The letter \"R\" appears once in \"Gian Maria\" and twice in \"Ambrogio\". Finally, both our names have the letter \"G\" in their composition.\n"
     ]
    }
   ],
   "source": [
    "context = start_chat(gpt35, \"you are an helpful ai\", \"Hi my name is Gian Maria. What is your name?\")\n",
    "print_chat(context)\n",
    "\n",
    "pprint(\"------------------\")\n",
    "# now you can prosecute the chat but you can change anything like the previous answer of the ai\n",
    "context[2].content = \"Hi Gian Maria, my name is Ambrogio.\"\n",
    "context = continue_chat(gpt35, context, \"Which are the letters that are common between our names? Please explain citing yours and my name.\")\n",
    "print_chat(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
