{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for AzureChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m _ \u001b[39m=\u001b[39m load_dotenv(find_dotenv()) \u001b[39m# read local .env file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# we cannot use a davinci we need a chat model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[39m=\u001b[39m AzureChatOpenAI(\n\u001b[1;32m      8\u001b[0m     openai_api_version\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m2023-03-15-preview\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     deployment_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGpt35\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     10\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-35-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.9/site-packages/langchain/load/serializable.py:64\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.9/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for AzureChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# we cannot use a davinci we need a chat model\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "torch.cuda.is_available() == False\n",
      "torch.backends.mps.is_enabled() == True\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "import torch\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())\n",
    "\n",
    "#verify availability of CUDA and print clearly \n",
    "print(f\"torch.cuda.is_available() == {torch.cuda.is_available()}\")\n",
    "# verify if mps is available\n",
    "print(f\"torch.backends.mps.is_enabled() == {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "\n",
    "# specify model to be used\n",
    "hf_model = \"Salesforce/blip-image-captioning-large\"\n",
    "# use GPU if it's available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if (device == 'cpu'):\n",
    "    # ok we are using CPU but we could use apple metal instead\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')  # use M1 chip if available\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessor will prepare images for the model\n",
    "processor = BlipProcessor.from_pretrained(hf_model)\n",
    "# then we initialize the model itself\n",
    "model = BlipForConditionalGeneration.from_pretrained(hf_model).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all `cache` is in folder \n",
    "\n",
    "```bash\n",
    "cd ~/.cache/huggingface/\n",
    "```\n",
    "\n",
    "Now try to classify an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "img_url = 'https://images.unsplash.com/photo-1616128417859-3a984dd35f02?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2372&q=80' \n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.9/site-packages/transformers/generation/utils.py:2396: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  if unfinished_sequences.max() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a monkey that is sitting in a tree\n"
     ]
    }
   ],
   "source": [
    "# unconditional image captioning\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=20)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a tool that can be used to classify an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "desc = (\n",
    "    \"use this tool when given the URL of an image that you'd like to be \"\n",
    "    \"described. It will return a simple caption describing the image.\"\n",
    "\n",
    ")\n",
    "\n",
    "class ImageCaptionTool(BaseTool):\n",
    "    name = \"Image captioner\"\n",
    "    description = desc\n",
    "    \n",
    "    def _run(self, url: str):\n",
    "        # download the image and convert to PIL object\n",
    "        image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "        # preprocess the image\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        # generate the caption\n",
    "        out = model.generate(**inputs, max_new_tokens=20)\n",
    "        # get the caption\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "tools = [ImageCaptionTool()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "# initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "        memory_key='chat_history',\n",
    "        k=5,\n",
    "        return_messages=True\n",
    ")\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = [ImageCaptionTool()]\n",
    "\n",
    "# initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask to explain the image using simply the url of the image, the chain will download the image and then classify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(f\"What does this image show?\\n{img_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://images.unsplash.com/photo-1502680390469-be75c86b636f?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=2370&q=80\"\n",
    "agent(f\"what is in this image?\\n{img_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
