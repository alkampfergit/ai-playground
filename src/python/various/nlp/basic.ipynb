{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = \"What is the name of the technique that consists in validating authorization of the user in each API request or in each call to a protected function?\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a simple NER for a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.620003</td>\n",
       "      <td>API</td>\n",
       "      <td>96</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score word  start  end\n",
       "0          ORG  0.620003  API     96   99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouped entity is deprecated.\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "output1 = ner_tagger(query)\n",
    "pd.DataFrame(output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.620003</td>\n",
       "      <td>API</td>\n",
       "      <td>96</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score word  start  end\n",
       "0          ORG  0.620003  API     96   99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Load the NER pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device, grouped_entities=True)\n",
    "output1 = nlp(query)\n",
    "pd.DataFrame(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length:  122\n",
      "How many words per tweet??\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"iarfmoose/t5-base-question-generator\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"iarfmoose/t5-base-question-generator\")\n",
    "\n",
    "# Define the input text\n",
    "answer = \"\"\"\n",
    "Transformer models have a maximum input sequence length that is\n",
    "referred to as the maximum context size. For applications using\n",
    "DistilBERT, the maximum context size is 512 tokens, which amounts\n",
    "to a few paragraphs of text. As we'll see in the next section, a token\n",
    "is an atomic piece of text; for now, we0ll treat a token as a single\n",
    "word. We can get a rough estimate of tweet lengths per emotion by\n",
    "looking at the distribution of words per tweet:\n",
    "\"\"\"\n",
    "context = \"Machine learning and Natural Language processing and huggingface.\"\n",
    "\n",
    "# Concatenate the answer and context\n",
    "input_text = f\"<answer> {answer} <context> {context}\"\n",
    "\n",
    "# Encode the input sequence\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "token_length = len(tokenizer.tokenize(input_text))\n",
    "print(\"Token length: \", token_length)\n",
    "# Generate questions\n",
    "output_ids = model.generate(input_ids)\n",
    "\n",
    "# Decode and print the questions\n",
    "questions = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(questions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to generate more than one question, I need to use beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length:  114\n",
      "['What is a complete mediation principle??', 'What is a complete mediation principle??????', 'What is a complete mediation principle???????']\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"iarfmoose/t5-base-question-generator\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"iarfmoose/t5-base-question-generator\")\n",
    "\n",
    "# Define the input text\n",
    "answer = \"\"\"\n",
    "Complete mediation is a security principle that requires every access to every object to be authorized. \n",
    "It means that access should be checked not only when a file is opened but also on each subsequent read or write to that file. \n",
    "This principle helps ensure that all accesses to objects are checked to ensure they are allowed. \n",
    "For example, whenever a subject attempts to read an object, the operating system should mediate the action by\n",
    " determining if the subject is allowed to read the object and providing the resources for the read to occur.\n",
    "\"\"\"\n",
    "answer = answer.replace('\\n', '')  # remove all carriage returns\n",
    "context = \"Computer security.\"\n",
    "\n",
    "# Concatenate the answer and context\n",
    "input_text = f\"<answer> {answer} <context> {context}\"\n",
    "\n",
    "# Encode the input sequence\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "token_length = len(tokenizer.tokenize(input_text))\n",
    "print(\"Token length: \", token_length)\n",
    "# Generate questions\n",
    "output_ids = model.generate(input_ids, num_beams=3, num_return_sequences=3)\n",
    "\n",
    "# Decode and print the questions\n",
    "questions = []\n",
    "for output in output_ids:\n",
    "    question = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    questions.append(question)\n",
    "print(questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "various",
   "language": "python",
   "name": "various"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
