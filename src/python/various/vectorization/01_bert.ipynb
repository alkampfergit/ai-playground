{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:\\temp\\cachetemphuggingface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes: 100%|██████████| 690/690 [00:00<?, ?B/s] \n",
      "Downloading 1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "Downloading 2_Dense/config.json: 100%|██████████| 114/114 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.58M/1.58M [00:00<00:00, 3.53MB/s]\n",
      "Downloading README.md: 100%|██████████| 2.47k/2.47k [00:00<?, ?B/s]\n",
      "Downloading config.json: 100%|██████████| 556/556 [00:00<?, ?B/s] \n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 539M/539M [01:00<00:00, 8.95MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 5.77MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 452/452 [00:00<?, ?B/s] \n",
      "Downloading vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 2.23MB/s]\n",
      "Downloading modules.json: 100%|██████████| 341/341 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0388562   0.01854845 -0.04066142 ...  0.01009196 -0.01660533\n",
      "  -0.00138948]\n",
      " [-0.00059496 -0.00924198 -0.05870508 ...  0.01638781  0.0150957\n",
      "  -0.04368325]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes: 100%|██████████| 1.18k/1.18k [00:00<?, ?B/s]\n",
      "Downloading 1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "Downloading README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.65MB/s]\n",
      "Downloading config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "Downloading data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 9.81MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:49<00:00, 8.92MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<?, ?B/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.44MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 363/363 [00:00<?, ?B/s] \n",
      "Downloading train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 13.0MB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.98MB/s]\n",
      "Downloading modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02250259 -0.07829171 -0.02303073 ... -0.00827929  0.02652687\n",
      "  -0.00201898]\n",
      " [ 0.04170235  0.00109742 -0.0155342  ... -0.02181627 -0.06359358\n",
      "  -0.00875287]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "transformers_cache = os.environ.get('TRANSFORMERS_CACHE')\n",
    "transformers_cache = \"s:\\\\temp\\\\cachetemphuggingface\" \n",
    "print(transformers_cache)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1', cache_folder=transformers_cache)\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "\n",
    "model2 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', cache_folder=transformers_cache)\n",
    "embeddings2 = model2.encode(sentences)\n",
    "print(embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has max length 128\n",
      "Model has max length 384\n",
      "['T_destination', '__add__', '__annotations__', '__call__', '__class__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_create_model_card', '_encode_multi_process_worker', '_eval_during_training', '_first_module', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_item_by_idx', '_get_name', '_get_scheduler', '_is_full_backward_hook', '_last_module', '_load_auto_model', '_load_from_state_dict', '_load_sbert_model', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_model_card_text', '_model_card_vars', '_model_config', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_checkpoint', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_target_device', '_text_length', '_version', 'add_module', 'append', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'device', 'double', 'dump_patches', 'encode', 'encode_multi_process', 'eval', 'evaluate', 'extend', 'extra_repr', 'fit', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_max_seq_length', 'get_parameter', 'get_sentence_embedding_dimension', 'get_sentence_features', 'get_submodule', 'half', 'insert', 'ipu', 'load', 'load_state_dict', 'max_seq_length', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'pop', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'save', 'save_to_hub', 'set_extra_state', 'share_memory', 'smart_batching_collate', 'start_multi_process_pool', 'state_dict', 'stop_multi_process_pool', 'to', 'to_empty', 'tokenize', 'tokenizer', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model has max length {model.max_seq_length}\")\n",
    "print(f\"Model has max length {model2.max_seq_length}\")\n",
    "\n",
    "# print(dir(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.3600, 0.4760, 0.1246],\n",
      "        [0.3600, 1.0000, 0.5832, 0.1339],\n",
      "        [0.4760, 0.5832, 1.0000, 0.1140],\n",
      "        [0.1246, 0.1339, 0.1140, 1.0000]])\n",
      "tensor([[ 1.0000,  0.4586,  0.6155,  0.0243],\n",
      "        [ 0.4586,  1.0000,  0.5901,  0.1052],\n",
      "        [ 0.6155,  0.5901,  1.0000, -0.0042],\n",
      "        [ 0.0243,  0.1052, -0.0042,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [\n",
    "    \"Yesterday I've played with my cat, I had a pleasant evening\", \n",
    "    \"I really like going to hike with my dog, it's a lot of fun\",\n",
    "    \"Swimming with my dog in the pool, really fun evening\",\n",
    "    \"I love elasticsearch capabilities to search for similar sentences\"]\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings2 = model2.encode(sentences)\n",
    "\n",
    "# Calculate the cosine similarity between the embeddings\n",
    "similarity_matrix = util.cos_sim(embeddings, embeddings)\n",
    "similarity_matrix2 = util.cos_sim(embeddings2, embeddings2)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Verify if you installed torch correctly and your GPU is available\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the name of the pre-trained transformer model to use\n",
    "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "\n",
    "# Define additional arguments to pass to the HuggingFaceEmbeddings constructor\n",
    "model_kwargs = {'device': 'cuda'} # or 'cpu'\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Create an instance of the HuggingFaceEmbeddings class using the specified model name and arguments\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is a Python script that demonstrates how to use the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module to create embeddings for text data using a pre-trained transformer model. \n",
    "\n",
    "First, the script imports the `HuggingFaceEmbeddings` class from the `langchain.embeddings` module. This class is used to create embeddings for text data using pre-trained transformer models from the Hugging Face model hub.\n",
    "\n",
    "Next, the script defines a `model_name` variable that specifies the name of the pre-trained transformer model to use. In this case, the model is `sentence-transformers/distiluse-base-multilingual-cased-v1`, which is a multilingual sentence embedding model based on the DistilBERT architecture.\n",
    "\n",
    "The script also defines two dictionaries: `model_kwargs` and `encode_kwargs`. These dictionaries are used to pass additional arguments to the `HuggingFaceEmbeddings` constructor. In this case, `model_kwargs` specifies that the model should be loaded onto the GPU if available, and `encode_kwargs` specifies that the embeddings should not be normalized.\n",
    "\n",
    "Finally, the script creates an instance of the `HuggingFaceEmbeddings` class using the `model_name`, `model_kwargs`, and `encode_kwargs` variables. This instance can then be used to encode text data into embeddings using the `encode` method.\n",
    "\n",
    "Overall, this code demonstrates how to use the `HuggingFaceEmbeddings` class to create embeddings for text data using a pre-trained transformer model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.11/site-packages/langchain/vectorstores/elastic_vector_search.py:143: UserWarning: ElasticVectorSearch will be removed in a future release. SeeElasticsearch integration docs on how to upgrade.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import elasticsearch python package. Please install it with `pip install elasticsearch`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.11/site-packages/langchain/vectorstores/elastic_vector_search.py:149\u001b[0m, in \u001b[0;36mElasticVectorSearch.__init__\u001b[0;34m(self, elasticsearch_url, index_name, embedding, ssl_verify)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39melasticsearch\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'elasticsearch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m \u001b[39mimport\u001b[39;00m ElasticVectorSearch\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m elastic_vector_search \u001b[39m=\u001b[39m ElasticVectorSearch(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     elasticsearch_url\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhttp://localhost:9201\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     index_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtest_index\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     embedding\u001b[39m=\u001b[39;49mhf\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gianmariaricci/develop/github/ai-playground/src/python/langchainVarious/vectorization/01_bert.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/develop/github/ai-playground/src/python/langchainVarious/langchain/lib/python3.11/site-packages/langchain/vectorstores/elastic_vector_search.py:151\u001b[0m, in \u001b[0;36mElasticVectorSearch.__init__\u001b[0;34m(self, elasticsearch_url, index_name, embedding, ssl_verify)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39melasticsearch\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import elasticsearch python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install elasticsearch`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m embedding\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_name \u001b[39m=\u001b[39m index_name\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import elasticsearch python package. Please install it with `pip install elasticsearch`."
     ]
    }
   ],
   "source": [
    "from langchain import ElasticVectorSearch\n",
    "\n",
    "elastic_vector_search = ElasticVectorSearch(\n",
    "    elasticsearch_url=\"http://localhost:9201\",\n",
    "    index_name=\"test_index\",\n",
    "    embedding=hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDF files: 100%|██████████| 18/18 [00:55<00:00,  3.09s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 PDF documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Define the path to the directory containing the PDF files\n",
    "pdf_dir = 'S:\\\\OneDrive\\\\Documentation\\\\HumbleBundle\\\\Security apress'\n",
    "\n",
    "# Create a list to store the loaded PDF documents\n",
    "pdf_docs = []\n",
    "\n",
    "# Traverse the directory tree and load the PDF files\n",
    "for root, dirs, files in os.walk(pdf_dir):\n",
    "    for file in tqdm(files, desc=\"Loading PDF files\", unit=\"file\"):\n",
    "        if file.endswith('.pdf'):  \n",
    "            pdf_path = os.path.join(root, file)\n",
    "            pdf_loader = PyPDFLoader(pdf_path)\n",
    "            pdf_doc = pdf_loader.load()\n",
    "            pdf_docs.append(pdf_doc)\n",
    "\n",
    "# Print the number of loaded PDF documents\n",
    "print(f\"Loaded {len(pdf_docs)} PDF documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Loaded 18 PDF documents'\n",
      "first doc has 239 elements\n",
      "firstElement is of type <class 'langchain.schema.Document'>\n",
      "firstElement has {'source': 'S:\\\\OneDrive\\\\Documentation\\\\HumbleBundle\\\\Security apress\\\\appliedcryptographyinnetandazurekeyvault.pdf', 'page': 0} medatata\n",
      "firstElement has Applied \n",
      "Cryptography in \n",
      ".NET and Azure  \n",
      "Key Vault\n",
      "A Practical Guide to Encryption in  \n",
      ".NET and .NET Core\n",
      "—\n",
      "Stephen Haunts\n",
      "Foreword by Troy Hunt page content\n",
      "Number of pages with more than 1000 characters: 4215\n",
      "Total number of pages: 6120\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(f\"Loaded {len(pdf_docs)} PDF documents\")\n",
    "doc = pdf_docs[0] \n",
    "print(f\"first doc has {len(doc)} elements\")\n",
    "firstElement = doc[0]\n",
    "print(f\"firstElement is of type {type(firstElement)}\")\n",
    "print(f\"firstElement has {firstElement.metadata} medatata\")\n",
    "print(f\"firstElement has {firstElement.page_content} page content\")\n",
    "\n",
    "total_pages = 0\n",
    "for doc in pdf_docs:\n",
    "    total_pages += len(doc)\n",
    "    # Count the number of pages that have more than 1000 characters\n",
    "    num_long_pages = 0\n",
    "    for doc in pdf_docs:\n",
    "        for page in doc:\n",
    "            if len(page.page_content) > 1000:\n",
    "                num_long_pages += 1\n",
    "\n",
    "print(f\"Number of pages with more than 1000 characters: {num_long_pages}\")\n",
    "print(f\"Total number of pages: {total_pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:301: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  version_num = client.info()[\"version\"][\"number\"][0]\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:306: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  client.indices.create(index=index_name, body={\"mappings\": mapping})\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:208: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  bulk(self.client, requests)\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:211: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.client.indices.refresh(index=self.index_name)\n",
      "c:\\develop\\github\\ai-playground\\src\\python\\langchainVarious\\langchain\\lib\\site-packages\\langchain\\vectorstores\\elastic_vector_search.py:189: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.client.indices.get(index=self.index_name)\n",
      "Created a chunk of size 1206, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'total number of embedded documents is 12257'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator = \" \", chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "number_of_docs = 0\n",
    "for doc in pdf_docs:\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "    number_of_docs += len(docs)\n",
    "    # print(f\"for document {doc[0].metadata} we have {len(docs)} embeddings from a total of {len(doc)} pages\")\n",
    "    db = elastic_vector_search.add_documents(docs)\n",
    "    # for chunk in text_splitter.split_documents(doc):\n",
    "    #     splitting.append(chunk)\n",
    "\n",
    "pprint(f\"total number of embedded documents is {number_of_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(separator = \" \",chunk_size=400, chunk_overlap=0)\n",
    "first_doc = pdf_docs[0] \n",
    "\n",
    "print(f\"first doc has {len(first_doc)} pages\")\n",
    "page = first_doc[50]\n",
    "print(f\"page number 50 has {len(page.page_content)} characters\")\n",
    "#pprint(page.page_content)\n",
    "\n",
    "page_chunks = text_splitter.split_text(page.page_content)\n",
    "print(f\"page has {len(page_chunks)} chunks\")\n",
    "pprint(page_chunks)\n",
    "\n",
    "# chunks = text_splitter.split_documents(first_doc)\n",
    "# print(f\"document has {len(chunks)} chunks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
