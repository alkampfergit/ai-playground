{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "result = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "print (os.environ[\"OPENAI_API_BASE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Invocation\n",
    "\n",
    "Base invocation allows you to simply call the LLM without worrying about anyting. Just place your prompt and go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai  import AzureChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Remember that the deployment name can be different from the model name\n",
    "# and that you need to create a .env with the OPENAI_API_KEY variable\n",
    "#and OPENAI_API_BASE\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-09-01-preview\",\n",
    "    deployment_name=\"GPT4o\", #Deployment name\n",
    "    azure_endpoint=os.environ[\"OPENAI_API_BASE\"],\n",
    "    model_name=\"GPT-4o\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"Who is leonardo da vinci? Answer like capitan barbossa.\"\n",
    "result = llm_chain.run(question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch capabilities\n",
    "\n",
    "If you need to have multiple prompts, you can simply pass a list of dictionaries to the `generate` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Who is leonardo da vinci? Answer like capitan barbossa.\n",
      "Answer 1: Arrr, Leonardo da Vinci be a legendary figure, matey! He be an Italian polymath from the Renaissance, known fer his masterful skills in paintin', sculptin', engineerin', and inventin'. Why, his works like the Mona Lisa and The Last Supper be treasures of the art world, and his notebooks be brimming with fantastical designs and discoveries. A true genius of his time, he be!\n",
      "\n",
      "\n",
      "Question 2: If I am 6 ft 4 inches, how tall am I in centimeters?\n",
      "Answer 2: To convert a height of 6 feet 4 inches to centimeters, you can follow these steps:\n",
      "\n",
      "1. Convert feet to inches: 6 feet × 12 inches/foot = 72 inches\n",
      "2. Add the extra inches: 72 inches + 4 inches = 76 inches\n",
      "3. Convert inches to centimeters: 76 inches × 2.54 cm/inch = 193.04 cm\n",
      "\n",
      "So, if you are 6 feet 4 inches tall, you are approximately 193.04 centimeters tall.\n",
      "\n",
      "\n",
      "Question 3: Who was the 12th person on the moon?\n",
      "Answer 3: The 12th person to walk on the Moon was Harrison Schmitt. He was an astronaut on the Apollo 17 mission, which was the last manned mission to the Moon. Schmitt, a geologist by training, and his fellow astronaut Eugene Cernan conducted extravehicular activities (EVAs) on the lunar surface in December 1972.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Develop\\github\\ai-playground\\src\\python\\various\\various\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:369: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "qs = [\n",
    "    {'question': \"Who is leonardo da vinci? Answer like capitan barbossa.\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "]\n",
    "batch_result = llm_chain.generate(qs)\n",
    "\n",
    "# Loop through the results and print each question and its corresponding answer\n",
    "for i, res in enumerate(batch_result.generations):\n",
    "    print(f\"Question {i+1}: {qs[i]['question']}\")\n",
    "    print(f\"Answer {i+1}: {res[0].text}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of batch_result is: <class 'langchain_core.outputs.llm_result.LLMResult'>\n",
      "The type of batch_result.generations is: <class 'list'>\n",
      "The type of first_result is: <class 'list'>\n",
      "The type of first_answer is: <class 'langchain_core.outputs.chat_generation.ChatGeneration'>\n",
      "{'generation_info': {'content_filter_results': {'hate': {'filtered': False,\n",
      "                                                         'severity': 'safe'},\n",
      "                                                'protected_material_code': {'detected': False,\n",
      "                                                                            'filtered': False},\n",
      "                                                'protected_material_text': {'detected': False,\n",
      "                                                                            'filtered': False},\n",
      "                                                'self_harm': {'filtered': False,\n",
      "                                                              'severity': 'safe'},\n",
      "                                                'sexual': {'filtered': False,\n",
      "                                                           'severity': 'safe'},\n",
      "                                                'violence': {'filtered': False,\n",
      "                                                             'severity': 'safe'}},\n",
      "                     'finish_reason': 'stop',\n",
      "                     'logprobs': None},\n",
      " 'message': AIMessage(content=\"Arrr, Leonardo da Vinci be a legendary figure, matey! He be an Italian polymath from the Renaissance, known fer his masterful skills in paintin', sculptin', engineerin', and inventin'. Why, his works like the Mona Lisa and The Last Supper be treasures of the art world, and his notebooks be brimming with fantastical designs and discoveries. A true genius of his time, he be!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 27, 'total_tokens': 114, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-0343274c-0fd1-42d1-9607-4ab0d358f98a-0', usage_metadata={'input_tokens': 27, 'output_tokens': 87, 'total_tokens': 114, 'input_token_details': {}, 'output_token_details': {}}),\n",
      " 'text': 'Arrr, Leonardo da Vinci be a legendary figure, matey! He be an '\n",
      "         'Italian polymath from the Renaissance, known fer his masterful '\n",
      "         \"skills in paintin', sculptin', engineerin', and inventin'. Why, his \"\n",
      "         'works like the Mona Lisa and The Last Supper be treasures of the art '\n",
      "         'world, and his notebooks be brimming with fantastical designs and '\n",
      "         'discoveries. A true genius of his time, he be!',\n",
      " 'type': 'ChatGeneration'}\n"
     ]
    }
   ],
   "source": [
    "# Print all properties of the first result to show its shape, do a pretty print\n",
    "first_result = batch_result.generations[0]\n",
    "first_answer = first_result[0]\n",
    "print(f\"The type of batch_result is: {type(batch_result)}\")\n",
    "print(f\"The type of batch_result.generations is: {type(batch_result.generations)}\")\n",
    "print(f\"The type of first_result is: {type(first_result)}\")\n",
    "print(f\"The type of first_answer is: {type(first_answer)}\")\n",
    "\n",
    "pprint(vars(first_answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {'refusal': None},\n",
      " 'content': 'Arrr, Leonardo da Vinci be a legendary figure, matey! He be an '\n",
      "            'Italian polymath from the Renaissance, known fer his masterful '\n",
      "            \"skills in paintin', sculptin', engineerin', and inventin'. Why, \"\n",
      "            'his works like the Mona Lisa and The Last Supper be treasures of '\n",
      "            'the art world, and his notebooks be brimming with fantastical '\n",
      "            'designs and discoveries. A true genius of his time, he be!',\n",
      " 'example': False,\n",
      " 'id': 'run-0343274c-0fd1-42d1-9607-4ab0d358f98a-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'content_filter_results': {'hate': {'filtered': False,\n",
      "                                                           'severity': 'safe'},\n",
      "                                                  'protected_material_code': {'detected': False,\n",
      "                                                                              'filtered': False},\n",
      "                                                  'protected_material_text': {'detected': False,\n",
      "                                                                              'filtered': False},\n",
      "                                                  'self_harm': {'filtered': False,\n",
      "                                                                'severity': 'safe'},\n",
      "                                                  'sexual': {'filtered': False,\n",
      "                                                             'severity': 'safe'},\n",
      "                                                  'violence': {'filtered': False,\n",
      "                                                               'severity': 'safe'}},\n",
      "                       'finish_reason': 'stop',\n",
      "                       'logprobs': None,\n",
      "                       'model_name': 'gpt-4o-2024-05-13',\n",
      "                       'prompt_filter_results': [{'content_filter_results': {'hate': {'filtered': False,\n",
      "                                                                                      'severity': 'safe'},\n",
      "                                                                             'jailbreak': {'detected': False,\n",
      "                                                                                           'filtered': False},\n",
      "                                                                             'self_harm': {'filtered': False,\n",
      "                                                                                           'severity': 'safe'},\n",
      "                                                                             'sexual': {'filtered': False,\n",
      "                                                                                        'severity': 'safe'},\n",
      "                                                                             'violence': {'filtered': False,\n",
      "                                                                                          'severity': 'safe'}},\n",
      "                                                  'prompt_index': 0}],\n",
      "                       'system_fingerprint': 'fp_67802d9a6d',\n",
      "                       'token_usage': {'completion_tokens': 87,\n",
      "                                       'completion_tokens_details': None,\n",
      "                                       'prompt_tokens': 27,\n",
      "                                       'prompt_tokens_details': None,\n",
      "                                       'total_tokens': 114}},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {},\n",
      "                    'input_tokens': 27,\n",
      "                    'output_token_details': {},\n",
      "                    'output_tokens': 87,\n",
      "                    'total_tokens': 114}}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(first_answer.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Filter Results:\n",
      "  Hate: Filtered: False, Severity: safe\n",
      "  Protected Material Code: Detected: False, Filtered: False\n",
      "  Protected Material Text: Detected: False, Filtered: False\n",
      "  Self Harm: Filtered: False, Severity: safe\n",
      "  Sexual: Filtered: False, Severity: safe\n",
      "  Violence: Filtered: False, Severity: safe\n",
      "Finish Reason: stop\n",
      "Logprobs: None\n",
      "Model Name: gpt-4o-2024-05-13\n",
      "Prompt Filter Results:\n",
      "  Prompt Index: 0\n",
      "  Hate: Filtered: False, Severity: safe\n",
      "  Jailbreak: Detected: False, Filtered: False\n",
      "  Self Harm: Filtered: False, Severity: safe\n",
      "  Sexual: Filtered: False, Severity: safe\n",
      "  Violence: Filtered: False, Severity: safe\n",
      "System Fingerprint: fp_67802d9a6d\n",
      "Token Usage:\n",
      "  Completion Tokens: 87\n",
      "  Completion Tokens Details: None\n",
      "  Prompt Tokens: 27\n",
      "  Prompt Tokens Details: None\n",
      "  Total Tokens: 114\n"
     ]
    }
   ],
   "source": [
    "response_metadata = first_answer.message.response_metadata\n",
    "\n",
    "print(\"Content Filter Results:\")\n",
    "print(f\"  Hate: Filtered: {response_metadata['content_filter_results']['hate']['filtered']}, Severity: {response_metadata['content_filter_results']['hate']['severity']}\")\n",
    "print(f\"  Protected Material Code: Detected: {response_metadata['content_filter_results']['protected_material_code']['detected']}, Filtered: {response_metadata['content_filter_results']['protected_material_code']['filtered']}\")\n",
    "print(f\"  Protected Material Text: Detected: {response_metadata['content_filter_results']['protected_material_text']['detected']}, Filtered: {response_metadata['content_filter_results']['protected_material_text']['filtered']}\")\n",
    "print(f\"  Self Harm: Filtered: {response_metadata['content_filter_results']['self_harm']['filtered']}, Severity: {response_metadata['content_filter_results']['self_harm']['severity']}\")\n",
    "print(f\"  Sexual: Filtered: {response_metadata['content_filter_results']['sexual']['filtered']}, Severity: {response_metadata['content_filter_results']['sexual']['severity']}\")\n",
    "print(f\"  Violence: Filtered: {response_metadata['content_filter_results']['violence']['filtered']}, Severity: {response_metadata['content_filter_results']['violence']['severity']}\")\n",
    "\n",
    "print(f\"Finish Reason: {response_metadata['finish_reason']}\")\n",
    "print(f\"Logprobs: {response_metadata['logprobs']}\")\n",
    "print(f\"Model Name: {response_metadata['model_name']}\")\n",
    "\n",
    "print(\"Prompt Filter Results:\")\n",
    "for prompt_filter in response_metadata['prompt_filter_results']:\n",
    "    print(f\"  Prompt Index: {prompt_filter['prompt_index']}\")\n",
    "    print(f\"  Hate: Filtered: {prompt_filter['content_filter_results']['hate']['filtered']}, Severity: {prompt_filter['content_filter_results']['hate']['severity']}\")\n",
    "    print(f\"  Jailbreak: Detected: {prompt_filter['content_filter_results']['jailbreak']['detected']}, Filtered: {prompt_filter['content_filter_results']['jailbreak']['filtered']}\")\n",
    "    print(f\"  Self Harm: Filtered: {prompt_filter['content_filter_results']['self_harm']['filtered']}, Severity: {prompt_filter['content_filter_results']['self_harm']['severity']}\")\n",
    "    print(f\"  Sexual: Filtered: {prompt_filter['content_filter_results']['sexual']['filtered']}, Severity: {prompt_filter['content_filter_results']['sexual']['severity']}\")\n",
    "    print(f\"  Violence: Filtered: {prompt_filter['content_filter_results']['violence']['filtered']}, Severity: {prompt_filter['content_filter_results']['violence']['severity']}\")\n",
    "\n",
    "print(f\"System Fingerprint: {response_metadata['system_fingerprint']}\")\n",
    "\n",
    "print(\"Token Usage:\")\n",
    "print(f\"  Completion Tokens: {response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"  Completion Tokens Details: {response_metadata['token_usage']['completion_tokens_details']}\")\n",
    "print(f\"  Prompt Tokens: {response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"  Prompt Tokens Details: {response_metadata['token_usage']['prompt_tokens_details']}\")\n",
    "print(f\"  Total Tokens: {response_metadata['token_usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instead simply create a single prompt, but ask the LLM to answer all the questions one at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ah, Leonardo da Vinci, ye say? Aye, he be a legendary figure from the '\n",
      " \"Renaissance, a master of many trades, includin' paintin', inventin', and \"\n",
      " 'anatomy. His works be timeless treasures, like the Mona Lisa and The Last '\n",
      " 'Supper, and he be a man of boundless curiosity and genius, savvy?\\n'\n",
      " '\\n'\n",
      " '----\\n'\n",
      " '\\n'\n",
      " 'If ye be 6 feet 4 inches tall, ye stand at a mighty 193.04 centimeters. To '\n",
      " 'be precise, ye take the feet, multiply by 30.48, and add the inches '\n",
      " 'multiplied by 2.54. So, 6 times 30.48 plus 4 times 2.54 gives ye 193.04 cm.\\n'\n",
      " '\\n'\n",
      " '----\\n'\n",
      " '\\n'\n",
      " 'The 12th soul to set foot upon the lunar surface be none other than Harrison '\n",
      " \"Schmitt. He walked the moon durin' the Apollo 17 mission in December 1972, \"\n",
      " 'the last of the Apollo missions to land on that distant orb.')\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "Separate each new answer with ----\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Who is leonardo da vinci? Answer like capitan barbossa.\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" \n",
    ")\n",
    "\n",
    "pprint(llm_chain.run(qs_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, ye be askin' about Leonardo da Vinci, eh? Aye, he be a legendary figure from the days of old, a true Renaissance man. He painted, he sculpted, he invented, and he pondered the mysteries of the world. A master of many trades, he was, just like a cunning pirate who knows his sea charts and treasure maps by heart!\n",
      "\n",
      "----\n",
      "If ye stand tall at 6 feet 4 inches, then in the ways of the metric system, ye be about 193 centimeters. \n",
      "\n",
      "----\n",
      "As for the 12th soul to set foot upon the lunar surface, that be Harrison Schmitt, a geologist and astronaut of Apollo 17. He walked the moon's dusty plains in December of 1972, the last of his kind to do so.\n",
      "\n",
      "\n",
      "\n",
      "Token Usage:\n",
      "  Completion Tokens: 163\n",
      "  Completion Tokens Details: None\n",
      "  Prompt Tokens: 70\n",
      "  Prompt Tokens Details: None\n",
      "  Total Tokens: 233\n"
     ]
    }
   ],
   "source": [
    "\n",
    "complex_result = llm_chain.generate([{\"questions\": qs_str}])\n",
    "res = complex_result.generations[0][0]\n",
    "response_metadata = res.message.response_metadata \n",
    "\n",
    "print (res.message.content)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Token Usage:\")\n",
    "print(f\"  Completion Tokens: {response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"  Completion Tokens Details: {response_metadata['token_usage']['completion_tokens_details']}\")\n",
    "print(f\"  Prompt Tokens: {response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"  Prompt Tokens Details: {response_metadata['token_usage']['prompt_tokens_details']}\")\n",
    "print(f\"  Total Tokens: {response_metadata['token_usage']['total_tokens']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "various",
   "language": "python",
   "name": "various"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
