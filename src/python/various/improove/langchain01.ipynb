{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alkopenai2.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "result = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "print (os.environ[\"OPENAI_API_BASE\"])\n",
    "langchain_tracing_v2 = os.environ.pop(\"LANGCHAIN_TRACING_V2\", None)\n",
    "langchain_endpoint = os.environ.pop(\"LANGCHAIN_ENDPOINT\", None)\n",
    "langchain_api_key = os.environ.pop(\"LANGCHAIN_API_KEY\", None)\n",
    "langchain_project = os.environ.pop(\"LANGCHAIN_PROJECT\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Invocation\n",
    "\n",
    "Base invocation allows you to simply call the LLM without worrying about anyting. Just place your prompt and go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/w60z7jfd3kggxf5884bbg2ym0000gn/T/ipykernel_48292/3208901216.py:24: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n",
      "/var/folders/5_/w60z7jfd3kggxf5884bbg2ym0000gn/T/ipykernel_48292/3208901216.py:31: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = llm_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there! Leonardo da Vinci, ye see, be a master of many trades, a true Renaissance man, if ye will. Born in the year of our Lord 1452 in the land of Italy, he be not just an artist, nay! He be a thinker, a scientist, a tinkerer of contraptions that'd make yer head spin faster than a ship caught in a maelstrom!\n",
      "\n",
      "With brush in hand, he painted the likes of the Mona Lisa, a lass with a smile that could charm the very sea itself. He sketched machines that could take flight, long before the likes of us ever dared to dream of soaring through the skies! Aye, he studied the stars, the human form, and the world around him like a captain charts the course of his ship.\n",
      "\n",
      "So, if ye be lookin' for brilliance, look no further than this fella, for he be a legend carved into the annals of time, a true treasure of knowledge and creativity! Savvy?\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai  import AzureChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# Remember that the deployment name can be different from the model name\n",
    "# and that you need to create a .env with the OPENAI_API_KEY variable\n",
    "#and OPENAI_API_BASE\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-09-01-preview\",\n",
    "    deployment_name=\"GPT-4o-mini\", #Deployment name\n",
    "    azure_endpoint=os.environ[\"OPENAI_API_BASE\"],\n",
    "    model_name=\"GPT-4o-mini\"\n",
    ")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# deprecated\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"Who is leonardo da vinci? Answer like capitan barbossa.\"\n",
    "result = llm_chain.run(question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation = prompt | llm\n",
    "response = invocation.invoke({\"question\": question})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch capabilities\n",
    "\n",
    "If you need to have multiple prompts, you can simply pass a list of dictionaries to the `generate` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "qs = [\n",
    "    {'question': \"Who is leonardo da vinci? Answer like capitan barbossa.\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "]\n",
    "batch_result = llm_chain.generate(qs)\n",
    "\n",
    "# Loop through the results and print each question and its corresponding answer\n",
    "for i, res in enumerate(batch_result.generations):\n",
    "    print(f\"Question {i+1}: {qs[i]['question']}\")\n",
    "    print(f\"Answer {i+1}: {res[0].text}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all properties of the first result to show its shape, do a pretty print\n",
    "first_result = batch_result.generations[0]\n",
    "first_answer = first_result[0]\n",
    "print(f\"The type of batch_result is: {type(batch_result)}\")\n",
    "print(f\"The type of batch_result.generations is: {type(batch_result.generations)}\")\n",
    "print(f\"The type of first_result is: {type(first_result)}\")\n",
    "print(f\"The type of first_answer is: {type(first_answer)}\")\n",
    "\n",
    "pprint(vars(first_answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vars(first_answer.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_metadata = first_answer.message.response_metadata\n",
    "\n",
    "print(\"Content Filter Results:\")\n",
    "print(f\"  Hate: Filtered: {response_metadata['content_filter_results']['hate']['filtered']}, Severity: {response_metadata['content_filter_results']['hate']['severity']}\")\n",
    "print(f\"  Protected Material Code: Detected: {response_metadata['content_filter_results']['protected_material_code']['detected']}, Filtered: {response_metadata['content_filter_results']['protected_material_code']['filtered']}\")\n",
    "print(f\"  Protected Material Text: Detected: {response_metadata['content_filter_results']['protected_material_text']['detected']}, Filtered: {response_metadata['content_filter_results']['protected_material_text']['filtered']}\")\n",
    "print(f\"  Self Harm: Filtered: {response_metadata['content_filter_results']['self_harm']['filtered']}, Severity: {response_metadata['content_filter_results']['self_harm']['severity']}\")\n",
    "print(f\"  Sexual: Filtered: {response_metadata['content_filter_results']['sexual']['filtered']}, Severity: {response_metadata['content_filter_results']['sexual']['severity']}\")\n",
    "print(f\"  Violence: Filtered: {response_metadata['content_filter_results']['violence']['filtered']}, Severity: {response_metadata['content_filter_results']['violence']['severity']}\")\n",
    "\n",
    "print(f\"Finish Reason: {response_metadata['finish_reason']}\")\n",
    "print(f\"Logprobs: {response_metadata['logprobs']}\")\n",
    "print(f\"Model Name: {response_metadata['model_name']}\")\n",
    "\n",
    "print(\"Prompt Filter Results:\")\n",
    "for prompt_filter in response_metadata['prompt_filter_results']:\n",
    "    print(f\"  Prompt Index: {prompt_filter['prompt_index']}\")\n",
    "    print(f\"  Hate: Filtered: {prompt_filter['content_filter_results']['hate']['filtered']}, Severity: {prompt_filter['content_filter_results']['hate']['severity']}\")\n",
    "    print(f\"  Jailbreak: Detected: {prompt_filter['content_filter_results']['jailbreak']['detected']}, Filtered: {prompt_filter['content_filter_results']['jailbreak']['filtered']}\")\n",
    "    print(f\"  Self Harm: Filtered: {prompt_filter['content_filter_results']['self_harm']['filtered']}, Severity: {prompt_filter['content_filter_results']['self_harm']['severity']}\")\n",
    "    print(f\"  Sexual: Filtered: {prompt_filter['content_filter_results']['sexual']['filtered']}, Severity: {prompt_filter['content_filter_results']['sexual']['severity']}\")\n",
    "    print(f\"  Violence: Filtered: {prompt_filter['content_filter_results']['violence']['filtered']}, Severity: {prompt_filter['content_filter_results']['violence']['severity']}\")\n",
    "\n",
    "print(f\"System Fingerprint: {response_metadata['system_fingerprint']}\")\n",
    "\n",
    "print(\"Token Usage:\")\n",
    "print(f\"  Completion Tokens: {response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"  Completion Tokens Details: {response_metadata['token_usage']['completion_tokens_details']}\")\n",
    "print(f\"  Prompt Tokens: {response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"  Prompt Tokens Details: {response_metadata['token_usage']['prompt_tokens_details']}\")\n",
    "print(f\"  Total Tokens: {response_metadata['token_usage']['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instead simply create a single prompt, but ask the LLM to answer all the questions one at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "Separate each new answer with ----\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = long_prompt | llm\n",
    "\n",
    "qs_str =  \"Who is leonardo da vinci? Answer like capitan barbossa.\\n\" + \\\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" + \\\n",
    "    \"Who was the 12th person on the moon?\" \n",
    "\n",
    "answer = llm_chain.invoke(qs_str)\n",
    "print(answer.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_metadata = answer.response_metadata\n",
    "\n",
    "print(\"Token Usage:\")\n",
    "print(f\"  Completion Tokens: {response_metadata['token_usage']['completion_tokens']}\")\n",
    "print(f\"  Completion Tokens Details: {response_metadata['token_usage']['completion_tokens_details']}\")\n",
    "print(f\"  Prompt Tokens: {response_metadata['token_usage']['prompt_tokens']}\")\n",
    "print(f\"  Prompt Tokens Details: {response_metadata['token_usage']['prompt_tokens_details']}\")\n",
    "print(f\"  Total Tokens: {response_metadata['token_usage']['total_tokens']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "def process_prompt(prompt: str):\n",
    "    model = llm\n",
    "    response = model.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.callbacks.openai_info import OpenAICallbackHandler\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(process_prompt).invoke(\"Who is leonardo da vinci? Answer like capitan barbossa.\")\n",
    "\n",
    "print(\"Output:\", output.content)\n",
    "attributes = [\n",
    "    'always_verbose', 'completion_tokens', 'ignore_agent', 'ignore_chain', 'ignore_chat_model', \n",
    "    'ignore_custom_event', 'ignore_llm', 'ignore_retriever', 'ignore_retry', 'prompt_tokens', 'raise_error', \n",
    "    'run_inline', 'successful_requests', 'total_cost', 'total_tokens'\n",
    "]\n",
    "print('\\n')\n",
    "formatted_total_cost = f\"${cb.total_cost:.8f}\"\n",
    "print(f\"total_cost: {formatted_total_cost}\")\n",
    "\n",
    "for attr in attributes:\n",
    "    print(f\"{attr}: {getattr(cb, attr)}\")\n",
    "#for attr in dir(cb):\n",
    "#    if not attr.startswith(\"__\"):\n",
    "#        print(f\"{attr}: {getattr(cb, attr)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "various",
   "language": "python",
   "name": "various"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
