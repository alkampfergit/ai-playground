{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on chat memory\n",
    "\n",
    "First part of the chain, just setup api key (on `azure` in this example). This cell prints the name of the base deployment just to verify that we correctly loaded the .env file.\n",
    "\n",
    "Remember you need a .env file with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxxxxx\n",
    "OPENAI_API_BASE=https://alkopenai2.openai.azure.com/\n",
    "HUGGINGFACEHUB_API_TOKEN=xxxxxxx\n",
    "PINECONE_KEY=xxxxxxx\n",
    "PINECONE_ENV=us-west1-gcp-free\n",
    "SERPAPI_API_KEY=xxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Remember that you need to set the OPENAI_API_BASE to point openai to your specific deployment.\n",
    "\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print base to verify you are pointint to the right deployment\n",
    "print(openai.api_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always I'm creating various llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI  \n",
    "\n",
    "# Pay attention that gpt35 and gpt4 are chat based llms\n",
    "gpt35 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "gpt4 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt4\", \n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "# this is not a Chat model this is a text model\n",
    "davinci = AzureOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"text-davinci-003\", \n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple helper\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def conversate(conversation, question):\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        result = conversation.predict(input= question)\n",
    "        print(cb)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a different memory, this time the memory is a  `ConversationBufferWindowMemory` that contains a parameter k to limit the number of interaction that are sent with the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from pprint import pprint\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k = 3) # k is the size of the window\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=gpt35,\n",
    "    memory=memory,\n",
    "    verbose=True #Feel free to set to true to look for internal working, in this example is not necessary\n",
    ")\n",
    "\n",
    "# Actually the conversation has some actual memory \n",
    "pprint(memory.load_memory_variables({}))\n",
    "\n",
    "result = conversation.predict(input = \"Hi my name is Gian Maria, what is your name?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of memory contains all the conversation but sends only the last k iterations to the LLM during the actual call. \n",
    "Continue to call the LLM after some iterations the original context seems to be lost and it does not know anymore my name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = conversation.predict(input = \"Can you do some basic math like 10+34?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input = \"Now 10 raised to the power of two?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input = \"Natural Logarithm of 100?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can verify, when you are talking with the LLM only the last k (in this situation is equal to 3) conversation are passed. But if you dump content of the memory you can verify that indeed the memory contains all the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint (f\"memory buffer length is: {len(memory.buffer)}\")\n",
    "pprint (f\"memory.chat_memory.messages length is: {len(memory.chat_memory.messages)}\")\n",
    "\n",
    "pprint (f\"memory.k is: {memory.k}\")\n",
    "# as you can see load_memory_variables is the method that actually loads the content to send to the llm\n",
    "pprint(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is quite evident that we can change the k whenever we want, changing the windows of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.k = 1 #make the memory size 1\n",
    "print(memory.load_memory_variables({}))\n",
    "memory.k = 2\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can ask to send all the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.k = 100\n",
    "conversate(conversation, \"What is my name? Please find this information from the beginning of the chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear() # same as previous one\n",
    "conversate(conversation, \"what is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a recap of the ConversationBufferwindowMemory class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_memory = ConversationBufferWindowMemory(k=2)\n",
    "test_memory.save_context({\"input\" : \"hello my name is Gian Maria\"}, {\"output\" : \"I'm an AI model and I'm here to help you\"})\n",
    "test_memory.save_context({\"input\" : \"What is my name?\"}, {\"output\" : \"Your name is Gian Maria\"})\n",
    "test_memory.save_context({\"input\" : \"Can you tell me the result of 1+1?\"}, {\"output\" : \"Sure the result is 2\"})\n",
    "\n",
    "print(f\"Buffer property has len: {len(test_memory.buffer)}\")\n",
    "print(f\"load_memory_variables property has len: {len(test_memory.load_memory_variables({}))}\")\n",
    "\n",
    "print(f\"load_memory_variables k = 2 => {test_memory.load_memory_variables({})}\")\n",
    "test_memory.k = 1\n",
    "print(f\"load_memory_variables with k = 1 => {test_memory.load_memory_variables({})}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory that keeps track of used tokens\n",
    "\n",
    "Having a memory that keep latest K messages is not so smart, better sometimes using token, because token is the real HARD limit of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 22 tokens in memory\n",
      "We have 203 tokens in memory\n",
      "Print the memory that will be pased to conversation\n",
      "{'history': \"Human: hello my name is Gian Maria\\nAI: I'm an AI model and I'm here to help you\\nHuman: What is my name?\\nAI: Your name is Gian Maria\\nHuman: can you tell me some lorem ipsum?\\nAI: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=davinci,  # since we are using a token limit we really need to specify the model used.\n",
    "    max_token_limit=300)\n",
    "\n",
    "memory.save_context({\"input\" : \"hello my name is Gian Maria\"}, {\"output\" : \"I'm an AI model and I'm here to help you\"})\n",
    "print(f\"We have {memory.llm.get_num_tokens_from_messages(memory.buffer)} tokens in memory\")\n",
    "\n",
    "memory.save_context({\"input\" : \"What is my name?\"}, {\"output\" : \"Your name is Gian Maria\"})\n",
    "memory.save_context({\"input\" : \"can you tell me some lorem ipsum?\"}, {\"output\" : \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\"})\n",
    "print(f\"We have {memory.llm.get_num_tokens_from_messages(memory.buffer)} tokens in memory\")\n",
    "\n",
    "print(\"Print the memory that will be pased to conversation\")\n",
    "print(memory.load_memory_variables({}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try to change the limit of the token but you will find that the content passed to the LLM does not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit is now 20 token - Actually we have 203 tokens in memory\n",
      "Print the memory that will be pased to conversation\n",
      "{'history': \"Human: hello my name is Gian Maria\\nAI: I'm an AI model and I'm here to help you\\nHuman: What is my name?\\nAI: Your name is Gian Maria\\nHuman: can you tell me some lorem ipsum?\\nAI: Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum\"}\n"
     ]
    }
   ],
   "source": [
    "memory.max_token_limit = 20\n",
    "print(f\"Limit is now {memory.max_token_limit} token - Actually we have {memory.llm.get_num_tokens_from_messages(memory.buffer)} tokens in memory\")\n",
    "print(\"Print the memory that will be pased to conversation\")\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessageHistory(messages=[HumanMessage(content='hello my name is Gian Maria', additional_kwargs={}, example=False), AIMessage(content=\"I'm an AI model and I'm here to help you\", additional_kwargs={}, example=False), HumanMessage(content='What is my name?', additional_kwargs={}, example=False), AIMessage(content='Your name is Gian Maria', additional_kwargs={}, example=False), HumanMessage(content='can you tell me some lorem ipsum?', additional_kwargs={}, example=False), AIMessage(content='Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum', additional_kwargs={}, example=False)])\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.chat_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This because the max_token_limit is not enforced when you read the buffer, but it is an hard limit of what is kept in memory, now that the limit is 20 tokens, we can simply add more content and the limit is enforced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: lets play\\nAI: Sure cool I like play'}\n",
      "[HumanMessage(content='lets play', additional_kwargs={}, example=False), AIMessage(content='Sure cool I like play', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "memory.save_context({\"input\" : \"lets play\"}, {\"output\" : \"Sure cool I like play\"})\n",
    "print(memory.load_memory_variables({}))\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **a strong `difference from the ConversationBufferWindowMemory class ` you cannot change number of tokens on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessageHistory(messages=[HumanMessage(content='lets play', additional_kwargs={}, example=False), AIMessage(content='Sure cool I like play', additional_kwargs={}, example=False)])\n"
     ]
    }
   ],
   "source": [
    "pprint(memory.chat_memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
