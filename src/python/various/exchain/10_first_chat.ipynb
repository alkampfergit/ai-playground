{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First chat\n",
    "\n",
    "First part of the chain, just setup api key (on `azure` in this example). This cell prints the name of the base deployment just to verify that we correctly loaded the .env file.\n",
    "\n",
    "Remember you need a .env file with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxxxxx\n",
    "OPENAI_API_BASE=https://alkopenai2.openai.azure.com/\n",
    "HUGGINGFACEHUB_API_TOKEN=xxxxxxx\n",
    "PINECONE_KEY=xxxxxxx\n",
    "PINECONE_ENV=us-west1-gcp-free\n",
    "SERPAPI_API_KEY=xxxxx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alkopenai2.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "# Remember that you need to set the OPENAI_API_BASE to point openai to your specific deployment.\n",
    "\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print base to verify you are pointint to the right deployment\n",
    "print(openai.api_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always I'm creating various llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI  \n",
    "\n",
    "# Pay attention that gpt35 and gpt4 are chat based llms\n",
    "gpt35 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt35\", \n",
    "    model_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "gpt4 =  AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"Gpt4\", \n",
    "    model_name=\"gpt-4\"\n",
    ")\n",
    "\n",
    "# this is not a Chat model this is a text model\n",
    "davinci = AzureOpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_version=\"2023-03-15-preview\",\n",
    "    deployment_name=\"text-davinci-003\", \n",
    "\tmodel_name=\"text-davinci-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the example starts to behave differently, we starts using a real `chat object` from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Memory for the conversation is chat_memory=ChatMessageHistory(messages=[]) '\n",
      " \"output_key=None input_key=None return_messages=False human_prefix='Human' \"\n",
      " \"ai_prefix='AI' memory_key='history'\")\n",
      "\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\"\n",
      "'--------------------------'\n",
      "' Your name is Gian Maria.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from pprint import pprint\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=davinci, # use davinci that is not a real chat model but it simply works.\n",
    "    verbose=False #Feel free to set to true to look for internal working, in this example is not necessary\n",
    ")\n",
    "\n",
    "# Actually the conversation has some actual memory\n",
    "pprint(f\"Memory for the conversation is {conversation.memory}\")\n",
    "\n",
    "result = conversation.predict(input = \"Hi my name is Gian Maria, what is your name?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we verified that the conversation has some memory, this means that the calls to the LLM is done using some form of memory. Now lets try **to dump some memory content to verify what is inside it**. As you can verify it contains simply the list of the messages, like previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi my name is Gian Maria, what is your name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\", additional_kwargs={}, example=False),\n",
      " HumanMessage(content='What is my name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=' Your name is Gian Maria.', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "pprint (conversation.memory.chat_memory.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to use a different form of `memory called ConversationBufferMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\"\n",
      "'--------------------------'\n",
      "' Your name is Gian Maria.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=davinci, # use davinci that is not a real chat model but it simply works.\n",
    "    memory=memory,\n",
    "    verbose=False #Feel free to set to true to look for internal working, in this example is not necessary\n",
    ")\n",
    "\n",
    "result = conversation.predict(input = \"Hi my name is Gian Maria, what is your name?\")\n",
    "pprint(result)\n",
    "pprint(\"--------------------------\")\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is pretty much the same, it seems that the ConverstaionBufferMemory works similarly to the ChatMessageHistory form of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi my name is Gian Maria, what is your name?', additional_kwargs={}, example=False), AIMessage(content=\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\", additional_kwargs={}, example=False), HumanMessage(content='What is my name?', additional_kwargs={}, example=False), AIMessage(content=' Your name is Gian Maria.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history')\n",
      "'------------------'\n",
      "[HumanMessage(content='Hi my name is Gian Maria, what is your name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=\" Hi Gian Maria, my name is AI-1. It's nice to meet you.\", additional_kwargs={}, example=False),\n",
      " HumanMessage(content='What is my name?', additional_kwargs={}, example=False),\n",
      " AIMessage(content=' Your name is Gian Maria.', additional_kwargs={}, example=False)]\n",
      "'------------------'\n",
      "('Human: Hi my name is Gian Maria, what is your name?\\n'\n",
      " \"AI:  Hi Gian Maria, my name is AI-1. It's nice to meet you.\\n\"\n",
      " 'Human: What is my name?\\n'\n",
      " 'AI:  Your name is Gian Maria.')\n"
     ]
    }
   ],
   "source": [
    "pprint (memory) #Please verify output, you can see that this wraps a ChatMessageHistory.\n",
    "pprint(\"------------------\")\n",
    "pprint (memory.chat_memory.messages)\n",
    "pprint(\"------------------\")\n",
    "pprint (memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we want to use a memory explicitly **instead of avoiding using the default one?**. One of the answer is the ability to tune the chat memory manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: Hi my name is Gian Maria, what is your name?\\n'\n",
      "            \"AI:  Hi Gian Maria, my name is AI-1. It's nice to meet you.\\n\"\n",
      "            'Human: What is my name?\\n'\n",
      "            'AI:  Your name is Gian Maria.'}\n"
     ]
    }
   ],
   "source": [
    "# you can get the history\n",
    "history =memory.load_memory_variables({})\n",
    "pprint (history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' Your name is John.'\n",
      "'Human: What is my name?\\nAI:  Your name is John.'\n"
     ]
    }
   ],
   "source": [
    "# i can reset the memory\n",
    "memory.clear()\n",
    "result = conversation.predict(input= \"What is my name?\")\n",
    "pprint(result)\n",
    "pprint(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Human: Hy my name is Gian Maria\\nAI: Hi Gian Maria my name is Alfred!'\n",
      "[HumanMessage(content='Hy my name is Gian Maria', additional_kwargs={}, example=False),\n",
      " AIMessage(content='Hi Gian Maria my name is Alfred!', additional_kwargs={}, example=False)]\n",
      "------------------\n",
      "'Your name is Gian Maria, and my name is Alfred.'\n"
     ]
    }
   ],
   "source": [
    "memory.clear()\n",
    "\n",
    "# I can preload the memory with a conversation, so we can start from a known state\n",
    "memory.save_context({\"input\" : \"Hy my name is Gian Maria\"}, {\"output\" : \"Hi Gian Maria my name is Alfred!\"})\n",
    "pprint(memory.buffer)\n",
    "pprint (memory.chat_memory.messages)\n",
    "\n",
    "print (\"------------------\")\n",
    "result = conversation.predict(input= \"What is my name? What is your name?\")\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I apologize for my previous response. Let me correct myself. The total '\n",
      " 'number of common letters in our names is 4. The unique common letters are '\n",
      " \"'a', 'r', 'i', and 'm'. Gian Maria and Alfred share these four letters.\")\n"
     ]
    }
   ],
   "source": [
    "result = conversation.predict(input= \"Sum the number of common letters of our names, give a detailed explanation of the answer.\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool stuff for this kind of interaction is that we can change anything **as an example I can change llm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hy my name is Gian Maria\n",
      "AI: Hi Gian Maria my name is Alfred!\n",
      "Human: What is my name? What is your name?\n",
      "AI: Your name is Gian Maria, and my name is Alfred.\n",
      "Human: Sum the number of common letters of our names, give a detailed explanation of the answer.\n",
      "AI: To sum the number of common letters in our names, we first need to list the unique letters in each of our names and then compare them to find the common ones. Here is the breakdown:\n",
      "\n",
      "Your name is Gian Maria:\n",
      "G, I, A, N, M, R\n",
      "\n",
      "My name is Alfred:\n",
      "A, L, F, R, E, D\n",
      "\n",
      "Now let's compare the unique letters of both names and find the common ones:\n",
      "\n",
      "Common letters: A, R\n",
      "\n",
      "There are 2 common letters between our names, which are A and R.\n",
      "Human: Sum the number of common letters of our names, give a detailed explanation of the answer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(\"I apologize for the confusion in my previous response. Let's try again to \"\n",
      " 'sum the number of common letters in our names:\\n'\n",
      " '\\n'\n",
      " 'Your name is Gian Maria:\\n'\n",
      " 'G, I, A, N, M, R\\n'\n",
      " '\\n'\n",
      " 'My name is Alfred:\\n'\n",
      " 'A, L, F, R, E, D\\n'\n",
      " '\\n'\n",
      " \"Now let's compare the unique letters of both names and find the common \"\n",
      " 'ones:\\n'\n",
      " '\\n'\n",
      " 'Common letters: A, R, and I (I missed I in my previous response)\\n'\n",
      " '\\n'\n",
      " 'There are 3 common letters between our names, which are A, R, and I.')\n",
      "Tokens Used: 374\n",
      "\tPrompt Tokens: 261\n",
      "\tCompletion Tokens: 113\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.01461\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    conversation.llm = gpt4 ##try using davinci\n",
    "    conversation.verbose = True\n",
    "    result = conversation.predict(input= \"Sum the number of common letters of our names, give a detailed explanation of the answer.\")\n",
    "    pprint(result)\n",
    "    print(cb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_experiments",
   "language": "python",
   "name": "langchain_experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
